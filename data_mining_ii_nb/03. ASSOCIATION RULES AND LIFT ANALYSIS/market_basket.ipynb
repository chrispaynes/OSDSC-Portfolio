{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75dc29a7-d9cd-452f-bb0d-08a3bb57db94",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4080ef-7bb5-45b3-b8de-e5b2a0480f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import harmonic_mean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                             roc_auc_score)\n",
    "from sklearn.model_selection import (GridSearchCV, RepeatedStratifiedKFold,\n",
    "                                     cross_val_score, train_test_split)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# suppress scientific notation in Pandas\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"precision\", 3)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 10]\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "\n",
    "sns.set()\n",
    "sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 1\n",
    "\n",
    "\n",
    "rnd = lambda x: round(x, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca485e19-db64-4f1d-8802-6f3f5cba9166",
   "metadata": {},
   "source": [
    "## Market Basket Analysis Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70205329-4657-4fc7-8484-280f1360babf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating rules with itertools\n",
    "\n",
    "from itertools import permutations\n",
    "# Extract unique items.\n",
    "flattened = [item for transaction in transactions for item in transaction]\n",
    "items = list(set(flattened))\n",
    "\n",
    "# Compute and print rules.\n",
    "rules = list(permutations(items, 2))\n",
    "print(rules)\n",
    "\n",
    "# Print the number of rules\n",
    "print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2119d-8a02-4a39-8ef3-3e76558a3ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the association rules function\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "# Compute frequent itemsets using the Apriori algorithm\n",
    "frequent_itemsets = apriori(onehot, min_support = 0.001,\n",
    "max_len = 2, use_colnames = True)\n",
    "# Compute all association rules for frequent_itemsets\n",
    "rules = association_rules(frequent_itemsets,\n",
    "metric = \"lift\",\n",
    "min_threshold = 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1627e1c1-59b2-4680-83f9-028ac18effb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "\n",
    "# Instantiate transaction encoder\n",
    "encoder = TransactionEncoder().fit(transactions)\n",
    "\n",
    "# One-hot encode itemsets by applying fit and transform\n",
    "onehot = encoder.transform(transactions)\n",
    "\n",
    "# Convert one-hot encoded data to DataFrame\n",
    "onehot = pd.DataFrame(onehot, columns = encoder.columns_)\n",
    "print(onehot)\n",
    "\n",
    "# Computing support for single items\n",
    "print(onehot.mean())\n",
    "\n",
    "# Computing support for multiple items\n",
    "import numpy as np\n",
    "# Define itemset that contains fiction and poetry\n",
    "onehot['fiction+poetry'] = np.logical_and(onehot['fiction'],onehot['poetry'])\n",
    "print(onehot.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8bba7d-07ca-4306-b370-30a61e361b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data\n",
    "from mlxtend.preprocessing import TransactionEncoder\n",
    "import pandas as pd\n",
    "# Split library strings into lists\n",
    "libraries = data['Library'].apply(lambda t: t.split(','))\n",
    "# Convert to list of lists\n",
    "libraries = list(libraries)\n",
    "# One-hot encode books\n",
    "books = TransactionEncoder().fit(libraries).transform(libraries)\n",
    "# Convert one-hot encoded data to DataFrame\n",
    "books = pd.DataFrame(books, columns = encoder.columns_)\n",
    "\n",
    "# Computing support.\n",
    "supportHG = np.logical_and(books['Hunger'],books['Gatsby']).mean()\n",
    "supportH = books['Hunger'].mean()\n",
    "supportG = books['Gatsby'].mean()\n",
    "\n",
    "# Compute and print confidence and lift.\n",
    "confidence = supportHG / supportH\n",
    "lift = supportHG / (supportH * supportG)\n",
    "\n",
    "# Print results.\n",
    "print(supportG, confidence, lift)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565462f9-b112-4a80-90f8-ff1557ea0081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apriori implementation\n",
    "# Import Apriori algorithm\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "# Load one-hot encoded novelty gifts data\n",
    "onehot = pd.read_csv('datasets/online_retail_onehot.csv')\n",
    "# Print header.\n",
    "print(onehot.head())\n",
    "\n",
    "# Compute frequent itemsets\n",
    "frequent_itemsets = apriori(onehot, min_support = 0.0005,\n",
    "max_len = 4, use_colnames = True)\n",
    "# Print number of itemsets\n",
    "print(len(frequent_itemsets))\n",
    "\n",
    "# Print itemsets\n",
    "print(frequent_itemsets.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461536bf-b55d-419e-b67e-5e0268633492",
   "metadata": {},
   "outputs": [],
   "source": [
    "How to compute association rules\n",
    "# Import Apriori algorithm\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "# Load one-hot encoded novelty gifts data\n",
    "onehot = pd.read_csv('datasets/online_retail_onehot.csv')\n",
    "# Apply Apriori algorithm\n",
    "frequent_itemsets = apriori(onehot,\n",
    "use_colnames=True,\n",
    "min_support=0.0001)\n",
    "# Compute association rules\n",
    "rules = association_rules(frequent_itemsets,\n",
    "metric = \"support\",\n",
    "min_threshold = 0.0)\n",
    "\n",
    "# The importance of pruning\n",
    "# Print the rules.\n",
    "print(rules)\n",
    "\n",
    "# Print the frequent itemsets.\n",
    "print(frequent_itemsets)\n",
    "\n",
    "# Compute association rules\n",
    "rules = association_rules(frequent_itemsets,\n",
    "metric = \"support\",\n",
    "min_threshold = 0.001)\n",
    "# Print the rules.\n",
    "print(rules)\n",
    "\n",
    "# Exploring the set of rules\n",
    "print(rules.columns)\n",
    "\n",
    "print(rules[['antecedents','consequents']])\n",
    "\n",
    "# Pruning with other metrics\n",
    "# Compute association rules\n",
    "rules = association_rules(frequent_itemsets,\n",
    "metric = \"antecedent support\",\n",
    "min_threshold = 0.002)\n",
    "# Print the number of rules.\n",
    "print(len(rules))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017068f8-92d6-48f0-a1b6-a8ec09ac482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating a heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899d9149-96d2-42ac-a96c-b70078212023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot Support versus confidence\n",
    "# What can we learn from scatterplots?\n",
    "    # Identify natural thresholds in data.\n",
    "    # Not possible with heatmaps or other visualizations.\n",
    "    # Visualize entire dataset.\n",
    "    # Not limited to small number of rules.\n",
    "    # Use  ndings to prune.\n",
    "    # Use natural thresholds and pa erns to prune."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6686bb0-8fe1-4f55-bdb3-bd5b8db4d3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parallel coordinate plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee90bdbd-d9b4-410f-a30e-5c51e4b97163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori\n",
    "from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "df = pd.read_excel('http://archive.ics.uci.edu/ml/machine-learning-databases/00352/Online%20Retail.xlsx')\n",
    "df.head()\n",
    "\n",
    "#  There is a little cleanup, we need to do. First, some of the descriptions have spaces that need to be removed. We’ll also drop the rows that don’t have invoice numbers and remove the credit transactions (those with invoice numbers containing C).\n",
    "df['Description'] = df['Description'].str.strip()\n",
    "df.dropna(axis=0, subset=['InvoiceNo'], inplace=True)\n",
    "df['InvoiceNo'] = df['InvoiceNo'].astype('str')\n",
    "df = df[~df['InvoiceNo'].str.contains('C')]\n",
    "\n",
    "# After the cleanup, we need to consolidate the items into 1 transaction per row with each product 1 hot encoded. For the sake of keeping the data set small, I’m only looking at sales for France. However, in additional code below, I will compare these results to sales from Germany. Further country comparisons would be interesting to investigate.\n",
    "basket = (df[df['Country'] ==\"France\"]\n",
    "          .groupby(['InvoiceNo', 'Description'])['Quantity']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('InvoiceNo'))\n",
    "\n",
    "# There are a lot of zeros in the data but we also need to make sure any positive values are converted to a 1 and anything less the 0 is set to 0. This step will complete the one hot encoding of the data and remove the postage column (since that charge is not one we wish to explore):\n",
    "\n",
    "def encode_units(x):\n",
    "    if x <= 0:\n",
    "        return 0\n",
    "    if x >= 1:\n",
    "        return 1\n",
    "\n",
    "basket_sets = basket.applymap(encode_units)\n",
    "basket_sets.drop('POSTAGE', inplace=True, axis=1)\n",
    "\n",
    "#  Now that the data is structured properly, we can generate frequent item sets that have a support of at least 7% (this number was chosen so that I could get enough useful examples):\n",
    "frequent_itemsets = apriori(basket_sets, min_support=0.07, use_colnames=True)\n",
    "\n",
    "\n",
    "# The final step is to generate the rules with their corresponding support, confidence and lift:\n",
    "rules = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1)\n",
    "rules.head()\n",
    "\n",
    "# That’s all there is to it! Build the frequent items using apriori then build the rules with association_rules .\n",
    "\n",
    "# Now, the tricky part is figuring out what this tells us. For instance, we can see that there are quite a few rules with a high lift value which means that it occurs more frequently than would be expected given the number of transaction and product combinations. We can also see several where the confidence is high as well. This part of the analysis is where the domain knowledge will come in handy. Since I do not have that, I’ll just look for a couple of illustrative examples.\n",
    "\n",
    "# We can filter the dataframe using standard pandas code. In this case, look for a large lift (6) and high confidence (.8):\n",
    "rules[ (rules['lift'] >= 6) &\n",
    "       (rules['confidence'] >= 0.8) ]\n",
    "\n",
    "# In looking at the rules, it seems that the green and red alarm clocks are purchased together and the red paper cups, napkins and plates are purchased together in a manner that is higher than the overall probability would suggest. At this point, you may want to look at how much opportunity there is to use the popularity of one product to drive sales of another. For instance, we can see that we sell 340 Green Alarm clocks but only 316 Red Alarm Clocks so maybe we can drive more Red Alarm Clock sales through recommendations?\n",
    "\n",
    "basket['ALARM CLOCK BAKELIKE GREEN'].sum()\n",
    "basket['ALARM CLOCK BAKELIKE RED'].sum()\n",
    "\n",
    "# What is also interesting is to see how the combinations vary by country of purchase. Let’s check out what some popular combinations might be in Germany:\n",
    "\n",
    "basket2 = (df[df['Country'] ==\"Germany\"]\n",
    "          .groupby(['InvoiceNo', 'Description'])['Quantity']\n",
    "          .sum().unstack().reset_index().fillna(0)\n",
    "          .set_index('InvoiceNo'))\n",
    "\n",
    "basket_sets2 = basket2.applymap(encode_units)\n",
    "basket_sets2.drop('POSTAGE', inplace=True, axis=1)\n",
    "frequent_itemsets2 = apriori(basket_sets2, min_support=0.05, use_colnames=True)\n",
    "rules2 = association_rules(frequent_itemsets2, metric=\"lift\", min_threshold=1)\n",
    "\n",
    "rules2[ (rules2['lift'] >= 4) &\n",
    "        (rules2['confidence'] >= 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80133de5-c5b4-473d-b8a8-e1a5f8dcb0df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ca0b82ad-7918-449d-b783-5d1f6c5d4013",
   "metadata": {
    "tags": []
   },
   "source": [
    "✅⭐👍👎\n",
    "\n",
    "### Scenario 1\n",
    "\n",
    "One of the most critical factors in customer relationship management that directly affects a company’s long-term profitability is understanding its customers. When a company can better understand its customer characteristics, it is better able to target products and marketing campaigns for customers, resulting in better profits for the company in the long term.\n",
    "\n",
    " \n",
    "\n",
    "You are an analyst for a telecommunications company that wants to better understand the characteristics of its customers. You have been asked to perform a market basket analysis to analyze customer data to identify key associations of your customer purchases, ultimately allowing better business and strategic decision-making.\n",
    "\n",
    " \n",
    "\n",
    "### Scenario 2\n",
    "\n",
    "One of the most critical factors in patient relationship management that directly affects a hospital’s long-term cost effectiveness is understanding its patients and the conditions leading to hospital admissions. When a hospital can better understand its patients’ characteristics, it is better able to target treatment to patients, resulting in more effective cost of care for the hospital in the long term.\n",
    "\n",
    " \n",
    "\n",
    "You are an analyst for a hospital that wants to better understand the characteristics of its patients. You have been asked to perform a market basket analysis to analyze patient data to identify key associations of your patients, ultimately allowing better business and strategic decision-making for the hospital.\n",
    "\n",
    "\n",
    "- I'm very frustrated and task 3. I cannot find the information needed to create these 119/120 columns. I have the rows. I honestly have no clue what I'm missing. I cannot find anything in the course materials that is helpful with this dataset.\n",
    "    - Hello Megan, Using R, the data needs to be converted to factor and transformed into a transactional dataset before using the Apriori algorithm Reminder. Please look up \"Transactionalizing the online data frame\" in chapter 2 of the datacamp material (Market Basket Analysis in R)\n",
    "\n",
    "- I'm working on Task 3 teleco MBA , does anyone have any tips or sources for how to clean the data? I understand we have to remove the blank rows which brings the count down to 7501 but I can't figure out how or why we are removing a column.\n",
    "    - Please see some tips below: 1. Read dataset into R 2. Explore data structure 3. Check for missing values 4. Remove empty rows or columns 5. Change data to factors 6. Verify that missing values are removed 7. Create a list 8. Create a dataframe for use with apriori 9.\n",
    "    - Your original mba data sets contained character vectors with categorical variables. Transforming your dataset from character vector to factor vector will create a suitable data structure for mba analysis. I encourage you to read more about character vector and factor vector, how to factor variables in R. \n",
    "    \n",
    "- I'm currently working on Task 3 and find the instructions for the task to be very vague and the datacamp material for basket analysis to be inadequate. What have others found helpful both in terms of finding the proper Python packages to use to satisfy the Task, but also getting a clear explanation for what is expected for the Task 3 analysis?\n",
    "    - Hello Nathan, Tips: 1. Follow the task overview and watch Video Resources in course “Announcements” 2. Schedule time with your CI if further clarifications are needed 3. Don’t forget to download either the teleco_market_basket.csv or medical_market_basket.csv from the Data Files and Associated Data Dictionary Files in task 3 4. Data transformation (cleaned data for analysis should be 7501 rows and 119 columns or items) 5. Joining the python community may help\n",
    "    \n",
    "- Hi cohort! - the \"teleco_market_basket\" csv for Task 3 seems to have a return resulting in a blank row b/t observations. Was that intentional, or is it just something we should adjust for on loading?\n",
    "    - This is intentional for the dataset provided.\n",
    "\n",
    "- Import Info: Please download the Market Basket Analysis or D212 task 3 dataset from the “Data Sets and Associated Data Dictionaries link in the introduction paragraph of Task 3. Market Basket Analysis must use transactional dataset to complete this task. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997a7a78-705f-4973-af25-c184832f29e8",
   "metadata": {},
   "source": [
    "# Part I: Research Question\n",
    "\n",
    "## A.  Describe the purpose of this data mining report by doing the following:\n",
    "\n",
    "### 1.  Propose one question relevant to a real-world organizational situation that you will answer using market basket analysis.\n",
    "\n",
    "    - One relevant question answered by using market basket analysis is\n",
    "        - Gibson Telecom wants to analyze customer data to identify key associations of your customer purchases, ultimately allowing better business and strategic decision-making\n",
    "        \n",
    "    - One plan to reduce churn is to offer customers discounts on items of interest. The execu tive team is not sure which items customers typically buy together and has reached out to you for help. You have been asked to analyze the data set to explore data on the purchase habits of customers .\n",
    "\n",
    "### 2.  Define one goal of the data analysis. Ensure that your goal is reasonable within the scope of the scenario and is represented in the available data.\n",
    "    - One data analysis goal is\n",
    "        - \n",
    "    - Identify services/add-ons frequently purchased together and construct customer service/add-on recommendation engine from the findings\n",
    "        - Identify items frequently purchased together\n",
    "    - Upsell products\n",
    "    - Cross-sell products\n",
    "    - Improve product recommendations\n",
    "    - offer items as a \"tech bundle\" or incentive to sign up for another service\n",
    "    - offer items for completing a survey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d48d9b67-90ed-4b94-8119-dc8cc76a0da1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c53d4ed-4fc5-4618-81b0-d8d3c94178e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0c5a2ffb-1e7c-41ec-9413-9be9a2e457a0",
   "metadata": {},
   "source": [
    "# Part II: Market Basket Justification\n",
    "\n",
    "## B.  Explain the reasons for using market basket analysis by doing the following:\n",
    "\n",
    "### 1.  Explain how market basket analyzes the selected dataset. Include expected outcomes.\n",
    "- Market basket analyzes the selected dataset by\n",
    "    - \n",
    "- The expected outcome is\n",
    "    - \n",
    "- Standard procedure for market basket analysis.\n",
    "    - 1. Generate large set of rules.\n",
    "        - Number of rules grows exponentially in number of items. Most rules are not useful.\n",
    "    - 2. Filter rules using metrics.\n",
    "        - Must apply initial round of filtering using Apriori algorithm\n",
    "    - 3. Apply intuition and common sense.\n",
    "- https://pbpython.com/market-basket-analysis.html    \n",
    "    - Association analysis is relatively light on the math concepts and easy to explain to non-technical people. In addition, it is an unsupervised learning tool that looks for hidden patterns so there is limited need for data prep and feature engineering. It is a good start for certain cases of data exploration and can point the way for a deeper dive into the data using other approaches.\n",
    "\n",
    "### 2.  Provide one example of transactions in the dataset.\n",
    "\n",
    "### 3.  Summarize one assumption of market basket analysis.\n",
    "- One assumption of market basket analysis is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c385a61-c21f-43be-9e67-275be96d6883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b021d9-4d7c-46d9-b652-2ff54d926033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaee557-57d6-407b-b755-1e962216a859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "79bb9705-ccf9-4170-906b-9d402b01ad94",
   "metadata": {},
   "source": [
    "# Part III: Data Preparation and Analysis\n",
    "\n",
    "## C.  Prepare and perform market basket analysis by doing the following:\n",
    "\n",
    "### 1.  Transform the dataset to make it suitable for market basket analysis. Include a copy of the cleaned dataset.\n",
    "- https://pbpython.com/market-basket-analysis.html\n",
    "    - One final note, related to the data. This analysis requires that all the data for a transaction be included in 1 row and the items should be 1-hot encoded. \n",
    "\n",
    "### 2.  Execute the code used to generate association rules with the Apriori algorithm. Provide screenshots that demonstrate the error-free functionality of the code.\n",
    "\n",
    "### 3.  Provide values for the support, lift, and confidence of the association rules table.\n",
    "\n",
    "### 4.  Identify the top three rules generated by the Apriori algorithm. Include a screenshot of the top rules along with their summaries.\n",
    "- DATACAMP\n",
    "    - Apriori principle.\n",
    "        - Subsets of frequent sets are frequent.\n",
    "        - Retain sets known to be frequent.\n",
    "        - Prune sets not known to be frequent.\n",
    "    - Apriori prunes itemsets.\n",
    "        - Applies minimum support threshold.\n",
    "        - Modified version can prune by number of items.\n",
    "        - Doesn't tell us about association rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6759b344-a413-4e7d-826d-03749cf08e8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854f34f7-2309-49f2-9d30-2fd976d6dbd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc6a53-a745-4c0e-9957-3e979119cf4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e17c54b4-7dea-4f55-8706-5ed226719408",
   "metadata": {},
   "source": [
    "\n",
    "# Part IV: Data Summary and Implications\n",
    "\n",
    "## D.  Summarize your data analysis by doing the following:\n",
    "\n",
    "### 1.  Summarize the significance of support, lift, and confidence from the results of the analysis.\n",
    "- https://pbpython.com/market-basket-analysis.html\n",
    "    - Support is the relative frequency that the rules show up. In many instances, you may want to look for high support in order to make sure it is a useful relationship. However, there may be instances where a low support is useful if you are trying to find “hidden” relationships.\n",
    "    - Confidence is a measure of the reliability of the rule. A confidence of .5 in the above example would mean that in 50% of the cases where Diaper and Gum were purchased, the purchase also included Beer and Chips. For product recommendation, a 50% confidence may be perfectly acceptable but in a medical situation, this level may not be high enough.\n",
    "    - Lift is the ratio of the observed support to that expected if the two rules were independent (see wikipedia). The basic rule of thumb is that a lift value close to 1 means the rules were completely independent. Lift values > 1 are generally more “interesting” and could be indicative of a useful rule pattern.\n",
    "- Support\n",
    "    - The significance of Support from the analysis results is\n",
    "    - DATACAMP\n",
    "        - The support metric measures the share of transactions that contain an itemset.\n",
    "            - number of transactions with items(s) / number of transactions\n",
    "- Lift\n",
    "    - The significance of Lift from the analysis results is\n",
    "    - DATACAMP\n",
    "        - Lift provides another metric for evaluating the relationship between items.\n",
    "        - Support(X&Y) / (Support(X) * Support(Y))\n",
    "            - Numerator: Proportion of transactions that contain X and Y.\n",
    "            - Denominator: Proportion if X and Y assigned randomly and independently.\n",
    "- Confidence\n",
    "    - The significance of Confidence from the analysis results is\n",
    "    - DATACAMP\n",
    "        - 1. Can improve over support with additional metrics.\n",
    "        - Adding con,dence provides a more complete picture.\n",
    "        - Support(X&Y) / Support(X)\n",
    "\n",
    "### 2.  Discuss the practical significance of the findings from the analysis.\n",
    "- The significance of the analysis findings is\n",
    "\n",
    "### 3.  Recommend a course of action for the real-world organizational situation from part A1 based on your results from part D1.\n",
    "- My recommended course of action based on the analysis findings is\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3800bff2-ff2d-4f94-9a81-7ebbffc96bc6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6dac8c83-e227-4bb3-b9e6-08f976b271a5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "034f47f2-99a4-44f1-bc16-22e27b9a66f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6ac465e-80da-4ef5-a49f-32a90fbb33c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part V: Attachments\n",
    "\n",
    "## E.  Provide a Panopto video recording that includes a demonstration of the functionality of the code used for the analysis and a summary of the programming environment.\n",
    " \n",
    "\n",
    "Note: The audiovisual recording should feature you visibly presenting the material (i.e., not in voiceover or embedded video) and should simultaneously capture both you and your multimedia presentation.\n",
    " \n",
    "\n",
    "Note: For instructions on how to access and use Panopto, use the \"Panopto How-To Videos\" web link provided below. To access Panopto's website, navigate to the web link titled \"Panopto Access,\" and then choose to log in using the “WGU” option. If prompted, log in using your WGU student portal credentials, and then it will forward you to Panopto’s website.\n",
    " \n",
    "\n",
    "To submit your recording, upload it to the Panopto drop box titled “Data Mining II – OFM3.” Once the recording has been uploaded and processed in Panopto's system, retrieve the URL of the recording from Panopto and copy and paste it into the Links option. Upload the remaining task requirements using the Attachments option.\n",
    " \n",
    "\n",
    "## F.  Record all web sources used to acquire data or segments of third-party code to support the application. Ensure the web sources are reliable.\n",
    "- https://pbpython.com/market-basket-analysis.html\n",
    "\n",
    "## G.  Acknowledge sources, using in-text citations and references, for content that is quoted, paraphrased, or summarized.\n",
    "- https://pbpython.com/market-basket-analysis.html\n",
    "\n",
    "## H.  Demonstrate professional communication in the content and presentation of your submission."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c63e9315-dc42-426c-a93e-ed22a1d4a28d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
