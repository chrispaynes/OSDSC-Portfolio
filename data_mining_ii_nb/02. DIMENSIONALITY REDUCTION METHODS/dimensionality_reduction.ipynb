{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50533d80-9f99-46ee-bc4c-334da207d597",
   "metadata": {},
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ad1746-69b2-43ec-be35-32d23809816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import harmonic_mean\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LassoCV, RidgeCV\n",
    "from sklearn.metrics import (classification_report, confusion_matrix,\n",
    "                             roc_auc_score)\n",
    "from sklearn.model_selection import (GridSearchCV, RepeatedStratifiedKFold,\n",
    "                                     cross_val_score, train_test_split)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# suppress scientific notation in Pandas\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"precision\", 3)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 10]\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "\n",
    "sns.set()\n",
    "sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 1\n",
    "\n",
    "\n",
    "rnd = lambda x: round(x, 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f96d0-3b42-42a2-a6ee-47d1a9f2f156",
   "metadata": {},
   "source": [
    "## PCA Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485e95eb-659b-47a1-b146-c03b1e53eb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "model = PCA()\n",
    "model.fit(samples)\n",
    "transformed = model.transform(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718e2dae-86bb-4243-95d3-b1aea55f8dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the variances of PCA features\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(samples)\n",
    "features = range(pca.n_components_)\n",
    "\n",
    "plt.bar(features, pca.explained_variance_)\n",
    "plt.xticks(features)\n",
    "plt.ylabel('variance')\n",
    "plt.xlabel('PCA feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c77a6fd-6c5f-4823-8e89-3bb5503df865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimension reduction of iris dataset\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(samples)\n",
    "transformed = pca.transform(samples)\n",
    "print(transformed.shape)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "xs = transformed[:,0]\n",
    "ys = transformed[:,1]\n",
    "plt.scatter(xs, ys, c=species)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b2f805-0ca0-46a7-8377-88874d7aad52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the principal components\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "std_df = scaler.fit_transform(df)\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "print(pca.fit_transform(std_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dda36d-90c2-44f6-a992-cd69aee29021",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(std_df)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab85b61-9ac9-4732-b207-f426f1fccdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(ansur_std_df)\n",
    "print(pca.explained_variance_ratio_.cumsum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e6c18c-92dc-47be-9371-521541af7f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pca.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76173abe-abfb-4b4d-96e7-850c7777e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "pipe = Pipeline([\n",
    "('scaler', StandardScaler()),\n",
    "('reducer', PCA())])\n",
    "pc = pipe.fit_transform(ansur_df)\n",
    "print(pc[:,:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af181fd-6fdc-40ad-92c5-3d931100ec66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the effect of categorical features\n",
    "ansur_categories['PC 1'] = pc[:,0]\n",
    "ansur_categories['PC 2'] = pc[:,1]\n",
    "sns.scatterplot(data=ansur_categories,\n",
    "x='PC 1', y='PC 2',\n",
    "hue='Height_class', alpha=0.4)\n",
    "\n",
    "sns.scatterplot(data=ansur_categories,\n",
    "x='PC 1', y='PC 2',\n",
    "hue='Gender', alpha=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c2182-b1b2-43e0-b44a-21280e070560",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "('scaler', StandardScaler()),\n",
    "('reducer', PCA(n_components=3)),\n",
    "('classifier', RandomForestClassifier())])\n",
    "print(pipe['reducer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0086804-8361-4127-ac75-4a4e9332ff29",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)\n",
    "pipe['reducer'].explained_variance_ratio_\n",
    "\n",
    "pipe['reducer'].explained_variance_ratio_.sum()\n",
    "\n",
    "print(pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17c272-20ab-4c3e-9cc4-3e87e245b694",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "('scaler', StandardScaler()),\n",
    "('reducer', PCA(n_components=0.9))])\n",
    "# Fit the pipe to the data\n",
    "pipe.fit(poke_df)\n",
    "print(len(pipe['reducer'].components_))\n",
    "\n",
    "pipe.fit(poke_df)\n",
    "var = pipe['reducer'].explained_variance_ratio_\n",
    "plt.plot(var)\n",
    "plt.xlabel('Principal component index')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1ba7a8-80af-4b34-9968-7fae50f43c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# split into training and testing sets\n",
    "X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3,\n",
    "    stratify=y, random_state=0\n",
    ")\n",
    "# standardize the features\n",
    "sc = StandardScaler()\n",
    "X_train_std = sc.fit_transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb91b558-3706-4652-b032-e3d7e2440de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using the numpy.cov function, we computed the covariance matrix of the standardized training dataset. Using the linalg.eig function, we performed the eigendecomposition, which yielded a vector (eigen_vals) consisting of 13 eigenvalues and the corresponding eigenvectors stored as columns in a 13 x 13-dimensional matrix (eigen_vecs).\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "cov_mat = np.cov(X_train_std.T)\n",
    "eigen_vals, eigen_vecs = np.linalg.eig(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdab695-b8ef-4853-b6db-6bc671c534b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# calculate cumulative sum of explained variances\n",
    "tot = sum(eigen_vals)\n",
    "var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)]\n",
    "cum_var_exp = np.cumsum(var_exp)\n",
    "\n",
    "# plot explained variances\n",
    "plt.bar(range(1,14), var_exp, alpha=0.5,\n",
    "        align='center', label='individual explained variance')\n",
    "plt.step(range(1,14), cum_var_exp, where='mid',\n",
    "         label='cumulative explained variance')\n",
    "plt.ylabel('Explained variance ratio')\n",
    "plt.xlabel('Principal component index')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4bfabb-0520-4b21-bb3e-be4a9a40c663",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We will sort the eigenpairs by descending order of the eigenvalues, construct a projection matrix from the selected eigenvectors, and use the projection matrix to transform the data onto the lower-dimensional subspace.\n",
    "\n",
    "We start by sorting the eigenpairs by decreasing order of the eigenvalues:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Make a list of (eigenvalue, eigenvector) tuples\n",
    "eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))]\n",
    "\n",
    "# Sort the (eigenvalue, eigenvector) tuples from high to low\n",
    "eigen_pairs.sort(key=lambda k: k[0], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a4d21d-ff60-4b42-a9b1-f1209d44af94",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Next, we collect the two eigenvectors that correspond to the two largest eigenvalues, to capture about 60% of the variance in this dataset. Note that we only chose two eigenvectors for the purpose of illustration, since we are going to plot the data via a two-dimensional scatter plot later in this subsection. In practice, the number of principal components has to be determined by a trade-off between computational efficiency and the performance of the classifier:\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis]))\n",
    "print('Matrix W:\\n', w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e66a0fae-1aae-4496-b4bd-b6e713bdb033",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "By executing the preceding code, we have created a 13 x 2-dimensional projection matrix W from the top two eigenvectors.\n",
    "\n",
    "Using the projection matrix, we can now transform a sample x (represented as a 1 x 13-dimensional row vector) onto the PCA subspace (the principal components one and two) obtaining x′, now a two-dimensional sample vector consisting of two new features:\n",
    "\"\"\"\n",
    "\n",
    "X_train_std[0].dot(w)\n",
    "\n",
    "# Similarly, we can transform the entire 124 x 13-dimensional training dataset onto the two principal components by calculating the matrix dot product:\n",
    "X_train_pca = X_train_std.dot(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b42f366-52ce-4dbf-8280-6b4b4f04e17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Lastly, let’s visualize the transformed Wine training set, now stored as an 124 x 2-dimensional matrix, in a two-dimensional scatterplot:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "colors = ['r', 'b', 'g']\n",
    "markers = ['s', 'x', 'o']\n",
    "for l, c, m in zip(np.unique(y_train), colors, markers):\n",
    "    plt.scatter(X_train_pca[y_train==l, 0], \n",
    "                X_train_pca[y_train==l, 1], \n",
    "                c=c, label=l, marker=m) \n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5266ec-3501-4383-8d7f-4389808b2895",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# intialize pca and logistic regression model\n",
    "pca = PCA(n_components=2)\n",
    "lr = LogisticRegression(multi_class='auto', solver='liblinear')\n",
    "\n",
    "# fit and transform data\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "lr.fit(X_train_pca, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8fae1-28ae-417b-8f99-9df1c7156cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "def plot_decision_regions(X, y, classifier, resolution=0.02):\n",
    "    # setup marker generator and color map\n",
    "    markers = ('s', 'x', 'o', '^', 'v')\n",
    "    colors = ('red', 'blue', 'lightgreen', 'gray', 'cyan')\n",
    "    cmap = ListedColormap(colors[:len(np.unique(y))])\n",
    "\n",
    "    # plot the decision surface\n",
    "    x1_min, x1_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "    x2_min, x2_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "    xx1, xx2 = np.meshgrid(np.arange(x1_min, x1_max, resolution),\n",
    "                           np.arange(x2_min, x2_max, resolution))\n",
    "    Z = classifier.predict(np.array([xx1.ravel(), xx2.ravel()]).T)\n",
    "    Z = Z.reshape(xx1.shape)\n",
    "    plt.contourf(xx1, xx2, Z, alpha=0.4, cmap=cmap)\n",
    "    plt.xlim(xx1.min(), xx1.max())\n",
    "    plt.ylim(xx2.min(), xx2.max())\n",
    "\n",
    "    # plot class samples\n",
    "    for idx, cl in enumerate(np.unique(y)):\n",
    "        plt.scatter(x=X[y == cl, 0], \n",
    "                    y=X[y == cl, 1],\n",
    "                    alpha=0.6, \n",
    "                    c=[cmap(idx)],\n",
    "                    edgecolor='black',\n",
    "                    marker=markers[idx], \n",
    "                    label=cl)# plot decision regions for training set\n",
    "\n",
    "\n",
    "plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16fba35-4dc4-49f0-8725-ea01689dcc60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot decision regions for test set\n",
    "plot_decision_regions(X_test_pca, y_test, classifier=lr)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend(loc='lower left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7926fc80-386c-400d-965c-461c2e0ecc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=None)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45ef5b1-627c-450f-905d-ffce3de5e31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c27c5b5-6834-4b21-8715-08e595f45b93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd67ffd9-1e48-434f-ab41-3d1b1256e484",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9301a4dc-82a6-4b33-861b-10b768b5a11d",
   "metadata": {
    "tags": []
   },
   "source": [
    "✅⭐👍👎\n",
    "\n",
    "### Scenario 1\n",
    "\n",
    "One of the most critical factors in customer relationship management that directly affects a company’s long-term profitability is understanding its customers. When a company can better understand its customer characteristics, it is better able to target products and marketing campaigns for customers, resulting in better profits for the company in the long term.\n",
    "\n",
    " \n",
    "\n",
    "You are an analyst for a telecommunications company that wants to better understand the characteristics of its customers. You have been asked to use principal component analysis (PCA) to analyze customer data to identify the principal variables of your customers, ultimately allowing better business and strategic decision-making.\n",
    "\n",
    " \n",
    "\n",
    "### Scenario 2\n",
    "\n",
    "One of the most critical factors in patient relationship management that directly affects a hospital’s long-term cost-effectiveness is understanding its patients and the conditions leading to hospital admissions. When a hospital can better understand its patients’ characteristics, it is better able to target treatment to patients, resulting in more effective cost of care for the hospital in the long term.\n",
    "\n",
    " \n",
    "\n",
    "You are an analyst for a hospital that wants to better understand the characteristics of its patients. You have been asked to use PCA to analyze patient data to identify the principal variables of your patients, ultimately allowing better business and strategic decision-making for the hospital."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38f1874-39c7-45d7-8dfe-8f14e8e30c6d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e03c6115-0c8c-4ff3-b807-7f5ac73d1b49",
   "metadata": {},
   "source": [
    "# Part I: Research Question\n",
    "\n",
    "## A.  Describe the purpose of this data mining report by doing the following:\n",
    "\n",
    "### 1.  Propose one question relevant to a real-world organizational situation that you will answer by using principal component analysis (PCA).\n",
    "\n",
    "### 2.  Define one goal of the data analysis. Ensure that your goal is reasonable within the scope of the scenario and is represented in the available data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9d32da-e2e7-485c-9587-3efab9fe3b3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e052fc-d6a6-495f-ab70-c3ef80444c83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce1951df-7b87-4e73-ad3d-701957b57d36",
   "metadata": {},
   "source": [
    "# Part II: Method Justification\n",
    "\n",
    "## B.  Explain the reasons for using PCA by doing the following:\n",
    "- \"Resulting PCA features are not linearly correlated\"\n",
    "- \"Represents same data, using less features\"\n",
    "- DATACAMP\n",
    "    - Your dataset will:\n",
    "        - be less complex\n",
    "        - require less disk space\n",
    "        - require less computation time\n",
    "        - have lower chance of model overfitting\n",
    "        - PCA removes correlation\n",
    "- https://ourarchive.otago.ac.nz/bitstream/handle/10523/7534/OUCS-2002-12.pdf?sequence=1&isAllowed=y\n",
    "    - PCA is a useful statistical technique that has found application in fields such as face recognition and image compression, and is a common technique for finding patterns in data of high dimension.\n",
    "\n",
    "### 1.  Explain how PCA analyzes the selected data set. Include expected outcomes.\n",
    "- \"Rows of transformed correspond to samples\"\n",
    "- \"Columns of transformed are the \"PCA features\"\n",
    "- \"Row gives PCA feature values of corresponding sample\"\n",
    "- \"Assumes the low variance features are \"noise\" and high variance features are informative\"\n",
    "- https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad\n",
    "    - Standardize the d-dimensional dataset.\n",
    "    - Construct the covariance matrix.\n",
    "    - Decompose the covariance matrix into its eigenvectors and eigenvalues.\n",
    "    - Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.\n",
    "    - Select k eigenvectors which correspond to the k largest eigenvalues, where k is the dimensionality of the new feature subspace (k ≤ d).\n",
    "    - Construct a projection matrix W from the “top” k eigenvectors.\n",
    "    - Transform the d-dimensional input dataset X using the projection matrix W to obtain the new k-dimensional feature subspace.\n",
    "    - The eigenvectors of the covariance matrix represent the principal components (the directions of maximum variance), whereas the corresponding eigenvalues will define their magnitude.\n",
    "- https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c\n",
    "    - Finally, we make an assumption that more variability in a particular direction correlates with explaining the behavior of the dependent variable. Lots of variability usually indicates signal, whereas little variability usually indicates noise. Thus, the more variability there is in a particular direction is, theoretically, indicative of something important we want to detect.\n",
    "    - Thus, PCA is a method that brings together:\n",
    "        - A measure of how each variable is associated with one another. (Covariance matrix.)\n",
    "        - The directions in which our data are dispersed. (Eigenvectors.)\n",
    "        - The relative importance of these different directions. (Eigenvalues.)\n",
    "        - PCA combines our predictors and allows us to drop the eigenvectors that are relatively unimportant.\n",
    "\n",
    "### 2.  Summarize one assumption of PCA.\n",
    "- DATACAMP\n",
    "- \"More effcient storage and computation\"\n",
    "- \"Remove less-informative \"noise\" features\"\n",
    "- \"Rotates data samples to be aligned with axes\"\n",
    "- \"Shifts data samples so they have mean 0\"\n",
    "- \"No information is lost\"\n",
    "- https://towardsdatascience.com/principal-component-analysis-for-dimensionality-reduction-115a3d157bad\n",
    "    - Note that the PCA directions are highly sensitive to data scaling, and we need to standardize the features prior to PCA if the features were measured on different scales and we want to assign equal importance to all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed61ded8-4e13-4ecc-b39e-827007be2fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47899407-cf7d-4351-9c2d-f0f771d6297f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2c56d1b7-287e-4894-8cbb-5145127a4e8a",
   "metadata": {},
   "source": [
    "# Part III: Data Preparation\n",
    "\n",
    "## C.  Perform data preparation for the chosen dataset by doing the following:\n",
    "\n",
    "### 1.  Identify the continuous dataset variables that you will need in order to answer the PCA question proposed in part A1.\n",
    "- If a Y variable exists and is part of your data, then separate your data into Y and X, as defined above — we’ll mostly be working with X.\n",
    "\n",
    "### 2.  Standardize the continuous dataset variables identified in part C1. Include a copy of the cleaned dataset.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db58d1bc-c660-4c27-9da4-ba6a10812c26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3f193e-b624-4b7c-aa09-816c091f44dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35a6457e-828f-4009-9f94-38d244217869",
   "metadata": {},
   "source": [
    "# Part IV: Analysis\n",
    "\n",
    "## D.  Perform PCA by doing the following:\n",
    "\n",
    "### 1.  Determine the matrix of all the principal components.\n",
    "\n",
    "### 2.  Identify the total number of principal components using the elbow rule or the Kaiser criterion. Include a screenshot of the scree plot.\n",
    "- Method 2: Calculate the proportion of variance explained (briefly explained below) for each feature, pick a threshold, and add features until you hit that threshold. (For example, if you want to explain 80% of the total variability possibly explained by your model, add features with the largest explained proportion of variance until your proportion of variance explained hits or exceeds 80%.)\n",
    "- Method 3: This is closely related to Method 2. Calculate the proportion of variance explained for each feature, sort features by proportion of variance explained and plot the cumulative proportion of variance explained as you keep more features. (This plot is called a scree plot, shown below.) One can pick how many features to include by identifying the point where adding a new feature has a significant drop in variance explained relative to the previous feature, and choosing features up until that point. (I call this the “find the elbow” method, as looking at the “bend” or “elbow” in the scree plot determines where the biggest drop in proportion of variance explained occurs.)\n",
    "\n",
    "### 3.  Identify the variance of each of the principal components identified in part D2.\n",
    "\n",
    "### 4.  Identify the total variance captured by the principal components identified in part D2.\n",
    "\n",
    "### 5.  Summarize the results of your data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a03d3-8685-4e1a-947f-3298a02c26f0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2e0fa-18b8-4225-b517-e0e1e6578c02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8066270-c2cd-40bc-b98a-5f7661ebcb02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735bc5c6-fece-413c-96e1-139200ebceab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8ac326-1d30-4f9e-b31d-18875ef1544b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265fe3c8-6c32-49c3-8836-4d49ed86a0eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4fb5295d-f14a-4062-a3f6-c2ce08ab4be3",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part V: Attachments\n",
    "\n",
    "## E.  Record the web sources used to acquire data or segments of third-party code to support the analysis. Ensure the web sources are reliable.\n",
    " \n",
    "\n",
    "### F.  Acknowledge sources, using in-text citations and references, for content that is quoted, paraphrased, or summarized.\n",
    "- https://www.youtube.com/watch?v=qtaqvPAeEJY\n",
    " \n",
    "\n",
    "### G.  Demonstrate professional communication in the content and presentation of your submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da93c673-4e2b-4371-85ca-5bd8bfe23219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad7cd12-a02d-44f9-8b13-51c37d275462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01bc0f1-e54f-4ed7-a96e-db49fe4da40a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af248ed5-3bfa-418a-8d0c-f564d416bb22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f6a622-81bd-47a6-abb1-125bdac7351d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925bef7b-e0d0-4932-991c-38fa196c9b21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
