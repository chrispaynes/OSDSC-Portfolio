{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75dc29a7-d9cd-452f-bb0d-08a3bb57db94",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69534511-9d11-4389-8816-af4520cd5f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 08:45:15.534102: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 08:45:15.711611: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-09-23 08:45:15.711638: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-09-23 08:45:15.744904: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-23 08:45:16.482341: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-23 08:45:16.482411: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-23 08:45:16.482419: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "ad4080ef-7bb5-45b3-b8de-e5b2a0480f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/chris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/chris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/chris/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/chris/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tensorflow import keras\n",
    "\n",
    "import re\n",
    "import string \n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "LEMMA = nltk.WordNetLemmatizer()\n",
    "STOP_WORDS = stopwords.words('english')\n",
    "\n",
    "\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# suppress scientific notation in Pandas\n",
    "pd.options.display.float_format = \"{:.2f}\".format\n",
    "pd.set_option(\"display.max_colwidth\", 5000)\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "# pd.set_option(\"precision\", 3)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = [12, 10]\n",
    "plt.rcParams[\"figure.dpi\"] = 150\n",
    "\n",
    "sns.set()\n",
    "sns.set_context(\"notebook\", rc={\"lines.linewidth\": 2.5})\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "RANDOM_STATE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1df44029-4043-4212-aa5a-4dcbdbd37834",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = pd.read_json(\"./src/amazon/reviews_Digital_Music_5.json\", lines=True)\n",
    "# df = original_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8341dade-70bf-4c62-946f-a33683b511d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(s:str) -> str:\n",
    "    # remove whitespace\n",
    "    s = s.strip()\n",
    "    \n",
    "    # lowercase\n",
    "    s = s.lower()\n",
    "\n",
    "    # create tokenize to tokenize on word characters\n",
    "    tokenizer = RegexpTokenizer('\\w+')    \n",
    "    # tokenize on word characters\n",
    "    tokens = tokenizer.tokenize(text=s)\n",
    "    \n",
    "    # lemmatize work\n",
    "    tokens = [LEMMA.lemmatize(word=word) for word in tokens]\n",
    "\n",
    "    # remove stop words     \n",
    "    tokens = [word for word in tokens if word not in STOP_WORDS]\n",
    "\n",
    "    # rejoin tokens into a string\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "def get_index(data):\n",
    "    t = tf.keras.preprocessing.text.Tokenizer(oov_token = '[UNK]', lower=True)\n",
    "    t.fit_on_texts(data)\n",
    "    index = t.word_index\n",
    "    \n",
    "    return len(index), index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "300d0d66-cde2-442d-873b-8001c3f7b7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: nltk in /home/chris/.local/lib/python3.10/site-packages (3.7)\n",
      "Requirement already satisfied: joblib in /home/chris/.local/lib/python3.10/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (8.0.3)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/chris/.local/lib/python3.10/site-packages (from nltk) (2022.9.13)\n",
      "Requirement already satisfied: tqdm in /home/chris/.local/lib/python3.10/site-packages (from nltk) (4.64.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/chris/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/chris/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/chris/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /home/chris/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "! pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a7c38927-ba81-407e-ba22-12c35bf29cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64706 entries, 0 to 64705\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   reviewerID      64706 non-null  object\n",
      " 1   asin            64706 non-null  object\n",
      " 2   reviewerName    64529 non-null  object\n",
      " 3   helpful         64706 non-null  object\n",
      " 4   reviewText      64706 non-null  object\n",
      " 5   overall         64706 non-null  int64 \n",
      " 6   summary         64706 non-null  object\n",
      " 7   unixReviewTime  64706 non-null  int64 \n",
      " 8   reviewTime      64706 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 4.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64706, 9)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         hard believe memory tree came 11 year ago ha held well passage time enya last great album new age pop amarantine day without rain back 1995 enya still creative spark voice agree reviewer said saddest album melancholy bittersweet opening title song memory tree elegaic majestic pax deorum sound like requiem mass dark threnody unlike reviewer said ha disconcerting blend spirituality sensuality find disconcerting anywhere hopeful song looking possibility hope ha place love listener decide romantic platonic etc always soft spot song way home triumphant ending return truly masterpiece new age music must enya fan\n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        clasically styled introverted album memory tree masterpiece subtlety many song endearing shyness soft piano lovely quiet voice within every introvert inferno enya let fire explode couple song absolutely burst expected raw power never heard enya might want start one popularized work like watermark play safe already fan collection complete without beautiful work musical art\n",
       "2                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  never thought enya would reach sublime height evacuee marble hall shepherd moon celt watermark day pleasant admirable throughout le ambitious lyrically musically hope ha place memory reach height beyond enya inspirational comforting actually glad song get overexposed way time make much special album\n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    third review irish album write today others cranberry sure ireland one country producing best music world commercial pop music spice girl way okay wanted say something irish music let say something album great beautiful good easy listened music like enya want easy listened relaxing music album buy\n",
       "4        enya despite successful recording artist broad appeal artist one reason might kind music pop station friendly another reason could people think macho music make people open mind could find beautiful melody harmony well lively upbeat tune joy put find memory tree one consistent album plus help ha probably one top 5 favorite song ever memory tree like record except recent amarantine album open wordless instrumental album title track probably favorite gentle piano arpeggio beautiful vocal track always like sparse use percussion 9 10anywhere lively single actually good sing along staccato beat great chorus also perfect length since 3 minute whereas pop song drag awhile one hook let go leaving wanting 9 10pax deorum another album another ominous latin sung piece music sound like something hear enemy territory video game enya give low voice almost monotone performance music slightly repetitive 2nd higher voice come piece hit stride halfway gorgeous bridge 8 10athair ar nearmh song doe song rather poppy catchy quality la time song similar another ballad best singing low key track also one album highlight 8 5 10from another instrumental one piano based piece would nice song learn pianist since technically complex fast pleasant listen 7 5 10china rose handful song could repeat never get sick one everything vocal background instrumentation lyric mention one relaxing almost tranquil ending coda ever heard definately favorite song plus whole career even 15 10hope ha place song china rose probably suffer tad song much granted vocal great always song ha nice relaxing mood either kind track familiar kind athair part 2 memorable 7 10tea house moon oriental instrument another wordless song although enjoyable mainly melody memorable bit song oriented different strangely likable 8 10once gold getting album final stretch problem one quieter affair feel album almost winding enya higher vocal reason reminiscent silent night rendition another beautiful track also bit 8 10la sonadora spanish lyric another slower paced track complete aahs background vocal synths album could benefited perkier track like doe halfway point give track little boost 7 5 10on way home church organ open track bouncy synth think wa bit upbeat lively vocal anywhere doe give album bit memorable closer especially chorus come track get fuller arrangement 9 10while day without rain watermark usually marked one best probably big hit time orinoco flow find memory tree likable album despite lack upbeat track still one finest outing\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  ...                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "64701                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   like reggae sound lot song heard radio really liked lot\n",
       "64702                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  first heard sirius fun song help sing long guy asking dad permission marry daughter told\n",
       "64703                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 absolutely love song downloaded fine would recommend song\n",
       "64704                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      reggae island beat really cup team wa particularly predisposed listening magic 34 rude 34 however gave chance found pleasantly enjoyable work magic take lazy way problem common modern music try coast reggae beat crafted fairly creative song guy hoping get girlfriend father permission ask hand marriage rudely shut father type airy substantive summer song get replayed later year good job\n",
       "64705                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            magic canadian band incorporates reggae pop sound 34 rude 34 play rude boy ha basic faceless pop band sound reggae beat floating top lead singer nasri atweh ha written song like justin bieber boyzone christina aguilera similar artist think artist song reggae beat instead pop beat 34 rude 34 like cookie cutter pop sound like 34 rude 34 clearly minority one song ha hit number one billboard hot 100\n",
       "Name: words, Length: 64706, dtype: object"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# DATA PREPATION STEPS\n",
    "# 1. Import data set as a data frame.\n",
    "prep_df = original_df.copy()\n",
    "\n",
    "# 2. Load packages for TensorFlow, Keras, and other libraries.\n",
    "\n",
    "# 3. EDA – to better understand the data structure.\n",
    "original_df.head(1)\n",
    "original_df.info()\n",
    "original_df.shape\n",
    "\n",
    "# 4. Remove unusual characters: uppercase letters, punctuation marks, emoticons, and non-English characters.\n",
    "prep_df[\"words\"] = prep_df.reviewText.apply(lambda x: clean_text(s=x))\n",
    "\n",
    "\n",
    "prep_df.words\n",
    "\n",
    "\n",
    "# pd.DataFrame(words)\n",
    "# words = re.split(r'\\W+', words.reviewText)\n",
    "# prep_df[\"words\"] = prep_df.words.str.split()\n",
    "# prep_df[\"words\"] = prep_df.words.str.split(r'\\W+')\n",
    "# prep_df[\"words\"]\n",
    "\n",
    "# prep_df.head(1)\n",
    "\n",
    "# 5. Measure the vocabulary size using (bags of words, etc.).\n",
    "\n",
    "\n",
    "# 6. Explore N-Gram (unigram, bigram, and trigram) structures for usefulness in text analysis as applicable.\n",
    "\n",
    "# 7. Determine the word embedding and explore embedded words based on target inputs.\n",
    "\n",
    "# 8. Set the maximum sequence length based on the length of the longest sentence in the data set.\n",
    "\n",
    "# 9. Tokenize sentences into a list of tokens and remove stop words.\n",
    "\n",
    "# 10. Pad cleaned sentences to fit the maximum sequence length after each text sequence.\n",
    "\n",
    "# 11. Create an activation function filled with dense layers of the neural network.\n",
    "\n",
    "# 12. Split the data set into training and test sets or into training/test/validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a58bedb0-46b4-434c-81d9-6df7b2259fbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewerID</th>\n",
       "      <th>asin</th>\n",
       "      <th>reviewerName</th>\n",
       "      <th>helpful</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>overall</th>\n",
       "      <th>summary</th>\n",
       "      <th>unixReviewTime</th>\n",
       "      <th>reviewTime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A3EBHHCZO6V2A4</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>Amaranth \"music fan\"</td>\n",
       "      <td>[3, 3]</td>\n",
       "      <td>It's hard to believe \"Memory of Trees\" came out 11 years ago;it has held up well over the passage of time.It's Enya's last great album before the New Age/pop of \"Amarantine\" and \"Day without rain.\" Back in 1995,Enya still had her creative spark,her own voice.I agree with the reviewer who said that this is her saddest album;it is melancholy,bittersweet,from the opening title song.\"Memory of Trees\" is elegaic&amp;majestic.;\"Pax Deorum\" sounds like it is from a Requiem Mass,it is a dark threnody.Unlike the reviewer who said that this has a \"disconcerting\" blend of spirituality&amp;sensuality;,I don't find it disconcerting at all.\"Anywhere is\" is a hopeful song,looking to possibilities.\"Hope has a place\" is about love,but it is up to the listener to decide if it is romantic,platonic,etc.I've always had a soft spot for this song.\"On my way home\" is a triumphant ending about return.This is truly a masterpiece of New Age music,a must for any Enya fan!</td>\n",
       "      <td>5</td>\n",
       "      <td>Enya's last great album</td>\n",
       "      <td>1158019200</td>\n",
       "      <td>09 12, 2006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AZPWAXJG9OJXV</td>\n",
       "      <td>5555991584</td>\n",
       "      <td>bethtexas</td>\n",
       "      <td>[0, 0]</td>\n",
       "      <td>A clasically-styled and introverted album, Memory of Trees is a masterpiece of subtlety.  Many of the songs have an endearing shyness to them - soft piano and a lovely, quiet voice.  But within every introvert is an inferno, and Enya lets that fire explode on a couple of songs that absolutely burst with an expected raw power.If you've never heard Enya before, you might want to start with one of her more popularized works, like Watermark, just to play it safe.  But if you're already a fan, then your collection is not complete without this beautiful work of musical art.</td>\n",
       "      <td>5</td>\n",
       "      <td>Enya at her most elegant</td>\n",
       "      <td>991526400</td>\n",
       "      <td>06 3, 2001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       reviewerID        asin          reviewerName helpful  \\\n",
       "0  A3EBHHCZO6V2A4  5555991584  Amaranth \"music fan\"  [3, 3]   \n",
       "1   AZPWAXJG9OJXV  5555991584             bethtexas  [0, 0]   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               reviewText  \\\n",
       "0  It's hard to believe \"Memory of Trees\" came out 11 years ago;it has held up well over the passage of time.It's Enya's last great album before the New Age/pop of \"Amarantine\" and \"Day without rain.\" Back in 1995,Enya still had her creative spark,her own voice.I agree with the reviewer who said that this is her saddest album;it is melancholy,bittersweet,from the opening title song.\"Memory of Trees\" is elegaic&majestic.;\"Pax Deorum\" sounds like it is from a Requiem Mass,it is a dark threnody.Unlike the reviewer who said that this has a \"disconcerting\" blend of spirituality&sensuality;,I don't find it disconcerting at all.\"Anywhere is\" is a hopeful song,looking to possibilities.\"Hope has a place\" is about love,but it is up to the listener to decide if it is romantic,platonic,etc.I've always had a soft spot for this song.\"On my way home\" is a triumphant ending about return.This is truly a masterpiece of New Age music,a must for any Enya fan!   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                          A clasically-styled and introverted album, Memory of Trees is a masterpiece of subtlety.  Many of the songs have an endearing shyness to them - soft piano and a lovely, quiet voice.  But within every introvert is an inferno, and Enya lets that fire explode on a couple of songs that absolutely burst with an expected raw power.If you've never heard Enya before, you might want to start with one of her more popularized works, like Watermark, just to play it safe.  But if you're already a fan, then your collection is not complete without this beautiful work of musical art.   \n",
       "\n",
       "   overall                   summary  unixReviewTime   reviewTime  \n",
       "0        5   Enya's last great album      1158019200  09 12, 2006  \n",
       "1        5  Enya at her most elegant       991526400   06 3, 2001  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 64706 entries, 0 to 64705\n",
      "Data columns (total 9 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   reviewerID      64706 non-null  object\n",
      " 1   asin            64706 non-null  object\n",
      " 2   reviewerName    64529 non-null  object\n",
      " 3   helpful         64706 non-null  object\n",
      " 4   reviewText      64706 non-null  object\n",
      " 5   overall         64706 non-null  int64 \n",
      " 6   summary         64706 non-null  object\n",
      " 7   unixReviewTime  64706 non-null  int64 \n",
      " 8   reviewTime      64706 non-null  object\n",
      "dtypes: int64(2), object(7)\n",
      "memory usage: 4.4+ MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(64706, 9)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1.\tAnalyzing the data and review length.\n",
    "\n",
    "original_df.head(2)\n",
    "original_df.info()\n",
    "original_df.shape\n",
    "\n",
    "# 2.\tCreating a dictionary and applying it to remove extraneous characters.\n",
    "# 3.\tDeciding on a typical review length.\n",
    "# 4.\tTokenization.\n",
    "# 5.\tApplying Tensorflow’s keras and layers methods\n",
    "# 6.\tSplitting the dataset into train and test\n",
    "# 7.\tFitting the model\n",
    "# 8.\tModel evaluation and reporting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610a0ff8-95c6-49d0-8752-2981bef9bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Task Two: Actually, it is pretty simple.  There are three text files, Amazon, IMDB, and Yelp.  You want to do a neural net to find the Sentiment index among the reviews.  These are the high-level ten basic steps:\n",
    "\n",
    "# 1.\tIf you run a couple of FOR loops to translate characters (essentially “cleaning up”).   There will be 52 unique characters remaining.\n",
    "# 2.\tPlot a histogram to find the length of the reviews. Most of the 500 reviews will be under 150 characters in length.\n",
    "# 3.\tCreate a dictionary for characters and tokens.\n",
    "# 4.\tUse arrays to hold each review and assign to it a list of one-hot-encoded characters.\n",
    "# 5.\tCode the output array to represent the review as positive (1) or negative (0).  The target value is binary.\n",
    "# 6.\tAdd terminators to the end of the data to fill the fixed length strings.\n",
    "# 7.\tUsing sklearn, numpy, and keras train a neural net.  Split data prior to training into test and validation.\n",
    "# 8.\tBy reading through the data in the network, weights are adjusted until errors are minimized.  Each period or cycle is referred to as an epoch.\n",
    "# 9.\tThen, you run a sequential neural net using keras.Sequential()\n",
    "# 10.\tAfter using the training process iteratively, the model.evaluate(x, y) will confirm the reliability. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd6d3f2-efc3-4498-8cad-5d0abb32237a",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "You have seen the power of using data analytical techniques to help organizations make data-driven decisions and now want to extend these models into areas of machine learning and artificial intelligence. In this task, you will explore the use of neural networks and natural language processing (NLP).\n",
    "\n",
    "In this task, you will choose a data file from the Web Links section. The available data sets are as follows:\n",
    "- [Amazon Product Data set](http://jmcauley.ucsd.edu/data/amazon/)\n",
    "- [UCSD Recommender Systems Data sets](https://cseweb.ucsd.edu/~jmcauley/datasets.html)\n",
    "- UCI Sentiment Labeled Sentences Data set\n",
    "\n",
    "For this task, you will build a neural network designed to learn word usage and context using NLP techniques. You will provide visualizations and a report, as well as build your network in an interactive development environment.\n",
    "\n",
    "In the telecom munications industry, customers can choose from multiple service providers and actively switch from one provider to another. Customer churn is defined as the percentage of customers who stopped using a provider’s product or service during a certain time frame. In this highly competitive market, some telecommunications industries can experience average annual churn rates as high as 25 Given that it costs 10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "\n",
    "For many providers retaining highly profitable customers is the number one business goal. To reduce customer churn, telecom munications companies need to predict which customers are at high risk of churn.\n",
    "\n",
    "As part of the “churn” project, executives would like to see consider a time series on revenue from the first years of operation. Once they understand any patterns in that data, they feel confident in understanding the impact of churn in current times. The given time series data records the daily revenue, in million dollars, during the first two years of operation.\n",
    "\n",
    "**Data File being used:**\n",
    "teleco_time_series.csv\n",
    "\n",
    "**Data Dictionary:**\n",
    "- The data set consists of 731 rows and two columns:\n",
    "    - Day Day during first two years of operation\n",
    "    - Revenue Revenue in million dollars\n",
    "    \n",
    "Review the data dictionary and considerations related to the raw data file you have chosen and prepare the data for time series modeling. You will then analyze that data set using time series modeling, create visualizations, generate forecasts, and deliver the results of your analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9841393-d7d9-4ae2-b604-80af93c1a091",
   "metadata": {},
   "source": [
    "# Part 0: Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf34752-167d-4c21-b0b6-37a4bad4d8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c3feed-01e1-45b2-9504-6dce3d32a384",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dbc80cb-6c1a-4c4d-ac76-34da4494fe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d39046-995c-4065-bb60-ea190db9e44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(df.isnull().sum(), columns=[\"# NaNs\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98c8ce-c3df-400a-b8c1-575939e3bebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{df.isnull().sum().sum():,} Total NaN Cells\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911d145f-87a1-4c68-a267-94d7a096017b",
   "metadata": {},
   "source": [
    "# Part I:  Research Question"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3b9d7f-6923-4a2a-b459-7b4e37631564",
   "metadata": {},
   "source": [
    "## A.  Describe the purpose of this data analysis by doing the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f1dbff-4f9d-4ef9-988f-4d7e71c1f01d",
   "metadata": {},
   "source": [
    "### 1.  Summarize one research question that you will answer using neural network models and NLP techniques. Be sure the research question is relevant to a real-world organizational situation and sentiment analysis captured in your chosen dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1d9260-01ed-4024-84dc-da128aa1f924",
   "metadata": {},
   "source": [
    "### 2.  Define the objectives or goals of the data analysis. Be sure the objectives or goals are reasonable within the scope of the research question and are represented in the available data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f1418-c2f2-46b5-958d-84e8ec214f09",
   "metadata": {},
   "source": [
    "### 3.  Identify a type of neural network capable of performing a text classification task that can be trained to produce useful predictions on text sequences on the selected data set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f14718-7f23-44a3-b40d-809f3fd0980f",
   "metadata": {},
   "source": [
    "# Part II:  Data Preparation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0eceba8-6baa-49f8-9aa4-2726ae8ca5fc",
   "metadata": {},
   "source": [
    "## B.  Summarize the data cleaning process by doing the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78df2325-4525-4669-b0f7-900749b75479",
   "metadata": {},
   "source": [
    "### 1.  Perform exploratory data analysis on the chosen dataset, and include an explanation of each of the following elements:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e6d5d4-3345-46df-968c-191a6beb8850",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e821fcc0-da0d-4249-a352-26704087690c",
   "metadata": {},
   "source": [
    "#### a. presence of unusual characters (e.g., emojis, non-English characters, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2cb43667-e5a4-4a2f-b3c7-c377947375ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                 {!, /, ', \", ., &, ,, ;}\n",
       "1                             {,, -, ', .}\n",
       "2                                {,, ', .}\n",
       "3                    {!, ', ., (, ,, ), -}\n",
       "4        {?, /, ', \", ., ,, (, ;, :, ), -}\n",
       "                       ...                \n",
       "64701                                  {.}\n",
       "64702                         {!, -, ', .}\n",
       "64703                               {,, .}\n",
       "64704          {!, ', ., &, ,, (, ;, #, )}\n",
       "64705          {!, ', ., &, (, ,, ;, #, )}\n",
       "Name: reviewText, Length: 64706, dtype: object"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_non_words(data):\n",
    "    non_word_tokenizer = RegexpTokenizer('[^\\w\\s]')    \n",
    "\n",
    "    # tokenize on non-word and non-space characters\n",
    "    return non_word_tokenizer.tokenize(text=data)\n",
    "\n",
    "original_df.reviewText.apply(lambda x: set(get_non_words(data=x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d410971c-e178-437c-b304-f5ce515b7cf8",
   "metadata": {},
   "source": [
    "#### b. vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "702890e5-835c-4058-9cb5-6a02bd9a18ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(108494,\n",
       " {'[UNK]': 1,\n",
       "  'album': 2,\n",
       "  'song': 3,\n",
       "  'quot': 4,\n",
       "  'wa': 5,\n",
       "  'one': 6,\n",
       "  'like': 7,\n",
       "  'track': 8,\n",
       "  'ha': 9,\n",
       "  'music': 10,\n",
       "  'good': 11,\n",
       "  'cd': 12,\n",
       "  'great': 13,\n",
       "  'sound': 14,\n",
       "  'time': 15,\n",
       "  'love': 16,\n",
       "  'best': 17,\n",
       "  'get': 18,\n",
       "  'first': 19,\n",
       "  'really': 20,\n",
       "  'band': 21,\n",
       "  'well': 22,\n",
       "  'rock': 23,\n",
       "  'still': 24,\n",
       "  'would': 25,\n",
       "  'make': 26,\n",
       "  'even': 27,\n",
       "  'much': 28,\n",
       "  'also': 29,\n",
       "  'lyric': 30,\n",
       "  'year': 31,\n",
       "  'fan': 32,\n",
       "  'new': 33,\n",
       "  'hit': 34,\n",
       "  'way': 35,\n",
       "  '5': 36,\n",
       "  'back': 37,\n",
       "  'beat': 38,\n",
       "  'vocal': 39,\n",
       "  'better': 40,\n",
       "  'know': 41,\n",
       "  'classic': 42,\n",
       "  'come': 43,\n",
       "  'think': 44,\n",
       "  'many': 45,\n",
       "  'record': 46,\n",
       "  'guitar': 47,\n",
       "  'single': 48,\n",
       "  'say': 49,\n",
       "  'two': 50,\n",
       "  'pop': 51,\n",
       "  'go': 52,\n",
       "  'another': 53,\n",
       "  'work': 54,\n",
       "  'thing': 55,\n",
       "  'listen': 56,\n",
       "  'favorite': 57,\n",
       "  'voice': 58,\n",
       "  'ever': 59,\n",
       "  'could': 60,\n",
       "  'never': 61,\n",
       "  'heard': 62,\n",
       "  'little': 63,\n",
       "  'release': 64,\n",
       "  'day': 65,\n",
       "  'take': 66,\n",
       "  'feel': 67,\n",
       "  '2': 68,\n",
       "  'people': 69,\n",
       "  'u': 70,\n",
       "  'every': 71,\n",
       "  'got': 72,\n",
       "  'though': 73,\n",
       "  'life': 74,\n",
       "  'b': 75,\n",
       "  '10': 76,\n",
       "  'doe': 77,\n",
       "  'artist': 78,\n",
       "  'version': 79,\n",
       "  'lot': 80,\n",
       "  'made': 81,\n",
       "  'something': 82,\n",
       "  'give': 83,\n",
       "  'last': 84,\n",
       "  'style': 85,\n",
       "  'rap': 86,\n",
       "  'star': 87,\n",
       "  'want': 88,\n",
       "  'bad': 89,\n",
       "  'hard': 90,\n",
       "  '4': 91,\n",
       "  'man': 92,\n",
       "  'long': 93,\n",
       "  'live': 94,\n",
       "  'world': 95,\n",
       "  'show': 96,\n",
       "  'big': 97,\n",
       "  'production': 98,\n",
       "  'always': 99,\n",
       "  'hear': 100,\n",
       "  '3': 101,\n",
       "  'released': 102,\n",
       "  'right': 103,\n",
       "  '1': 104,\n",
       "  'nice': 105,\n",
       "  'put': 106,\n",
       "  'end': 107,\n",
       "  'see': 108,\n",
       "  'pretty': 109,\n",
       "  'since': 110,\n",
       "  'title': 111,\n",
       "  'disc': 112,\n",
       "  'original': 113,\n",
       "  'collection': 114,\n",
       "  'bit': 115,\n",
       "  'find': 116,\n",
       "  'soul': 117,\n",
       "  'top': 118,\n",
       "  'different': 119,\n",
       "  'blue': 120,\n",
       "  'may': 121,\n",
       "  'real': 122,\n",
       "  'listening': 123,\n",
       "  'start': 124,\n",
       "  'debut': 125,\n",
       "  'ballad': 126,\n",
       "  'let': 127,\n",
       "  'old': 128,\n",
       "  'solo': 129,\n",
       "  'beautiful': 130,\n",
       "  'r': 131,\n",
       "  'buy': 132,\n",
       "  'going': 133,\n",
       "  'tune': 134,\n",
       "  'second': 135,\n",
       "  'almost': 136,\n",
       "  'hip': 137,\n",
       "  'yet': 138,\n",
       "  'however': 139,\n",
       "  'probably': 140,\n",
       "  'hop': 141,\n",
       "  'part': 142,\n",
       "  'quite': 143,\n",
       "  'around': 144,\n",
       "  'singer': 145,\n",
       "  'musical': 146,\n",
       "  'nothing': 147,\n",
       "  'play': 148,\n",
       "  'next': 149,\n",
       "  'without': 150,\n",
       "  'far': 151,\n",
       "  'enough': 152,\n",
       "  'need': 153,\n",
       "  'group': 154,\n",
       "  'radio': 155,\n",
       "  'set': 156,\n",
       "  'girl': 157,\n",
       "  'whole': 158,\n",
       "  'actually': 159,\n",
       "  'greatest': 160,\n",
       "  'away': 161,\n",
       "  'three': 162,\n",
       "  'came': 163,\n",
       "  'must': 164,\n",
       "  'cover': 165,\n",
       "  'amazing': 166,\n",
       "  'side': 167,\n",
       "  'excellent': 168,\n",
       "  'worth': 169,\n",
       "  'point': 170,\n",
       "  'kind': 171,\n",
       "  'although': 172,\n",
       "  'review': 173,\n",
       "  'true': 174,\n",
       "  'recording': 175,\n",
       "  'seems': 176,\n",
       "  'heart': 177,\n",
       "  'sure': 178,\n",
       "  'line': 179,\n",
       "  'dance': 180,\n",
       "  'catchy': 181,\n",
       "  'thought': 182,\n",
       "  'along': 183,\n",
       "  'done': 184,\n",
       "  'rest': 185,\n",
       "  'melody': 186,\n",
       "  'definitely': 187,\n",
       "  'word': 188,\n",
       "  'early': 189,\n",
       "  'overall': 190,\n",
       "  'fact': 191,\n",
       "  'number': 192,\n",
       "  'perfect': 193,\n",
       "  'high': 194,\n",
       "  'piano': 195,\n",
       "  'keep': 196,\n",
       "  'minute': 197,\n",
       "  'full': 198,\n",
       "  'singing': 199,\n",
       "  'cut': 200,\n",
       "  'said': 201,\n",
       "  'guy': 202,\n",
       "  'chorus': 203,\n",
       "  'anything': 204,\n",
       "  'strong': 205,\n",
       "  '8': 206,\n",
       "  'mind': 207,\n",
       "  'quality': 208,\n",
       "  'jazz': 209,\n",
       "  'john': 210,\n",
       "  'might': 211,\n",
       "  'boy': 212,\n",
       "  'game': 213,\n",
       "  'together': 214,\n",
       "  'material': 215,\n",
       "  'produced': 216,\n",
       "  'look': 217,\n",
       "  'playing': 218,\n",
       "  'place': 219,\n",
       "  'fun': 220,\n",
       "  '9': 221,\n",
       "  'country': 222,\n",
       "  'everything': 223,\n",
       "  'le': 224,\n",
       "  'tell': 225,\n",
       "  'especially': 226,\n",
       "  'rapper': 227,\n",
       "  'bass': 228,\n",
       "  'stand': 229,\n",
       "  'stuff': 230,\n",
       "  'hot': 231,\n",
       "  'maybe': 232,\n",
       "  'funk': 233,\n",
       "  'night': 234,\n",
       "  'least': 235,\n",
       "  'later': 236,\n",
       "  'career': 237,\n",
       "  'amp': 238,\n",
       "  '34': 239,\n",
       "  'mean': 240,\n",
       "  'feature': 241,\n",
       "  'today': 242,\n",
       "  'note': 243,\n",
       "  'young': 244,\n",
       "  'found': 245,\n",
       "  'drum': 246,\n",
       "  'black': 247,\n",
       "  'enjoy': 248,\n",
       "  'truly': 249,\n",
       "  'talent': 250,\n",
       "  'elton': 251,\n",
       "  'effort': 252,\n",
       "  '80': 253,\n",
       "  'change': 254,\n",
       "  'head': 255,\n",
       "  'others': 256,\n",
       "  '50': 257,\n",
       "  'white': 258,\n",
       "  'woman': 259,\n",
       "  'reason': 260,\n",
       "  'name': 261,\n",
       "  'hook': 262,\n",
       "  'used': 263,\n",
       "  'course': 264,\n",
       "  'making': 265,\n",
       "  'mix': 266,\n",
       "  'moment': 267,\n",
       "  'cool': 268,\n",
       "  'deep': 269,\n",
       "  'recorded': 270,\n",
       "  'interesting': 271,\n",
       "  '7': 272,\n",
       "  'featuring': 273,\n",
       "  'money': 274,\n",
       "  'already': 275,\n",
       "  'anyone': 276,\n",
       "  'heavy': 277,\n",
       "  'written': 278,\n",
       "  'n': 279,\n",
       "  'slow': 280,\n",
       "  'story': 281,\n",
       "  'bought': 282,\n",
       "  'lead': 283,\n",
       "  'previous': 284,\n",
       "  'half': 285,\n",
       "  'verse': 286,\n",
       "  'eminem': 287,\n",
       "  'solid': 288,\n",
       "  'late': 289,\n",
       "  'wonderful': 290,\n",
       "  'g': 291,\n",
       "  'jay': 292,\n",
       "  'studio': 293,\n",
       "  'believe': 294,\n",
       "  'piece': 295,\n",
       "  'light': 296,\n",
       "  'simply': 297,\n",
       "  'try': 298,\n",
       "  'rather': 299,\n",
       "  'feeling': 300,\n",
       "  'flow': 301,\n",
       "  'getting': 302,\n",
       "  '6': 303,\n",
       "  'friend': 304,\n",
       "  'listener': 305,\n",
       "  'close': 306,\n",
       "  'highly': 307,\n",
       "  'else': 308,\n",
       "  'short': 309,\n",
       "  'either': 310,\n",
       "  'dark': 311,\n",
       "  'played': 312,\n",
       "  'past': 313,\n",
       "  'sing': 314,\n",
       "  'home': 315,\n",
       "  '70': 316,\n",
       "  'hand': 317,\n",
       "  'power': 318,\n",
       "  'recommend': 319,\n",
       "  'everyone': 320,\n",
       "  'left': 321,\n",
       "  'wonder': 322,\n",
       "  'video': 323,\n",
       "  'f': 324,\n",
       "  'turn': 325,\n",
       "  'personal': 326,\n",
       "  'lp': 327,\n",
       "  'remember': 328,\n",
       "  'four': 329,\n",
       "  'wrong': 330,\n",
       "  'unique': 331,\n",
       "  'metal': 332,\n",
       "  'couple': 333,\n",
       "  'seem': 334,\n",
       "  'highlight': 335,\n",
       "  'matter': 336,\n",
       "  'performance': 337,\n",
       "  'opinion': 338,\n",
       "  'sounding': 339,\n",
       "  'bonus': 340,\n",
       "  'yes': 341,\n",
       "  'musician': 342,\n",
       "  'often': 343,\n",
       "  'check': 344,\n",
       "  'masterpiece': 345,\n",
       "  'sings': 346,\n",
       "  'someone': 347,\n",
       "  'producer': 348,\n",
       "  'pick': 349,\n",
       "  'perhaps': 350,\n",
       "  'e': 351,\n",
       "  'five': 352,\n",
       "  'certainly': 353,\n",
       "  'awesome': 354,\n",
       "  'folk': 355,\n",
       "  'trying': 356,\n",
       "  'stop': 357,\n",
       "  'roll': 358,\n",
       "  'fire': 359,\n",
       "  'arrangement': 360,\n",
       "  'z': 361,\n",
       "  'add': 362,\n",
       "  'tight': 363,\n",
       "  'fine': 364,\n",
       "  'went': 365,\n",
       "  'acoustic': 366,\n",
       "  'baby': 367,\n",
       "  'known': 368,\n",
       "  'street': 369,\n",
       "  'call': 370,\n",
       "  'listened': 371,\n",
       "  'michael': 372,\n",
       "  'gone': 373,\n",
       "  'hope': 374,\n",
       "  'help': 375,\n",
       "  'brilliant': 376,\n",
       "  'lost': 377,\n",
       "  'looking': 378,\n",
       "  'dre': 379,\n",
       "  'riff': 380,\n",
       "  'king': 381,\n",
       "  'dream': 382,\n",
       "  'open': 383,\n",
       "  'hold': 384,\n",
       "  'sometimes': 385,\n",
       "  'finally': 386,\n",
       "  'funky': 387,\n",
       "  'simple': 388,\n",
       "  'become': 389,\n",
       "  'jam': 390,\n",
       "  'instead': 391,\n",
       "  'run': 392,\n",
       "  'called': 393,\n",
       "  'paul': 394,\n",
       "  'coming': 395,\n",
       "  'god': 396,\n",
       "  'american': 397,\n",
       "  'eye': 398,\n",
       "  'groove': 399,\n",
       "  'self': 400,\n",
       "  'liked': 401,\n",
       "  'genre': 402,\n",
       "  'c': 403,\n",
       "  'easy': 404,\n",
       "  'success': 405,\n",
       "  'sweet': 406,\n",
       "  'type': 407,\n",
       "  'guest': 408,\n",
       "  'hearing': 409,\n",
       "  'follow': 410,\n",
       "  'several': 411,\n",
       "  'include': 412,\n",
       "  'jackson': 413,\n",
       "  'took': 414,\n",
       "  'rhythm': 415,\n",
       "  'oh': 416,\n",
       "  'death': 417,\n",
       "  'problem': 418,\n",
       "  'sense': 419,\n",
       "  'including': 420,\n",
       "  'huge': 421,\n",
       "  'loved': 422,\n",
       "  'throughout': 423,\n",
       "  'era': 424,\n",
       "  'opening': 425,\n",
       "  'chart': 426,\n",
       "  'entire': 427,\n",
       "  'talk': 428,\n",
       "  'theme': 429,\n",
       "  'na': 430,\n",
       "  'use': 431,\n",
       "  'case': 432,\n",
       "  'face': 433,\n",
       "  'incredible': 434,\n",
       "  'beginning': 435,\n",
       "  'p': 436,\n",
       "  'influence': 437,\n",
       "  'instrumental': 438,\n",
       "  'fantastic': 439,\n",
       "  'example': 440,\n",
       "  'powerful': 441,\n",
       "  'straight': 442,\n",
       "  'doubt': 443,\n",
       "  'easily': 444,\n",
       "  'happy': 445,\n",
       "  'player': 446,\n",
       "  'included': 447,\n",
       "  'mr': 448,\n",
       "  'punk': 449,\n",
       "  'member': 450,\n",
       "  'alone': 451,\n",
       "  'ago': 452,\n",
       "  'fit': 453,\n",
       "  'break': 454,\n",
       "  'songwriter': 455,\n",
       "  '12': 456,\n",
       "  'club': 457,\n",
       "  'commercial': 458,\n",
       "  'hate': 459,\n",
       "  'brother': 460,\n",
       "  'lady': 461,\n",
       "  'third': 462,\n",
       "  'ok': 463,\n",
       "  'songwriting': 464,\n",
       "  '90': 465,\n",
       "  'special': 466,\n",
       "  'popular': 467,\n",
       "  'touch': 468,\n",
       "  'intro': 469,\n",
       "  'rocker': 470,\n",
       "  'sort': 471,\n",
       "  'tempo': 472,\n",
       "  'rhyme': 473,\n",
       "  'west': 474,\n",
       "  'fresh': 475,\n",
       "  'die': 476,\n",
       "  'fast': 477,\n",
       "  'school': 478,\n",
       "  'similar': 479,\n",
       "  'child': 480,\n",
       "  'emotion': 481,\n",
       "  'car': 482,\n",
       "  'torus': 483,\n",
       "  'age': 484,\n",
       "  'label': 485,\n",
       "  'wish': 486,\n",
       "  'completely': 487,\n",
       "  'move': 488,\n",
       "  'form': 489,\n",
       "  'among': 490,\n",
       "  'writing': 491,\n",
       "  'miss': 492,\n",
       "  'mary': 493,\n",
       "  'became': 494,\n",
       "  'started': 495,\n",
       "  'saying': 496,\n",
       "  'key': 497,\n",
       "  'stone': 498,\n",
       "  'experience': 499,\n",
       "  'decent': 500,\n",
       "  'idea': 501,\n",
       "  'lil': 502,\n",
       "  'string': 503,\n",
       "  'fall': 504,\n",
       "  'despite': 505,\n",
       "  'kid': 506,\n",
       "  'somewhat': 507,\n",
       "  'art': 508,\n",
       "  'mood': 509,\n",
       "  'melodic': 510,\n",
       "  'pure': 511,\n",
       "  'sample': 512,\n",
       "  'absolutely': 513,\n",
       "  'earlier': 514,\n",
       "  'wanted': 515,\n",
       "  'anyway': 516,\n",
       "  'jones': 517,\n",
       "  'smooth': 518,\n",
       "  'house': 519,\n",
       "  'remix': 520,\n",
       "  'free': 521,\n",
       "  'wait': 522,\n",
       "  'behind': 523,\n",
       "  'lyrical': 524,\n",
       "  'talented': 525,\n",
       "  'job': 526,\n",
       "  'able': 527,\n",
       "  'instrument': 528,\n",
       "  'etc': 529,\n",
       "  'madonna': 530,\n",
       "  'bring': 531,\n",
       "  'party': 532,\n",
       "  'musically': 533,\n",
       "  'sad': 534,\n",
       "  'gave': 535,\n",
       "  'keyboard': 536,\n",
       "  'mid': 537,\n",
       "  'gem': 538,\n",
       "  'background': 539,\n",
       "  'concert': 540,\n",
       "  'gangsta': 541,\n",
       "  'attention': 542,\n",
       "  'modern': 543,\n",
       "  'lover': 544,\n",
       "  'copy': 545,\n",
       "  'hell': 546,\n",
       "  'upbeat': 547,\n",
       "  'level': 548,\n",
       "  'weak': 549,\n",
       "  'memorable': 550,\n",
       "  'future': 551,\n",
       "  'care': 552,\n",
       "  'dont': 553,\n",
       "  'given': 554,\n",
       "  'decade': 555,\n",
       "  'understand': 556,\n",
       "  'chance': 557,\n",
       "  'harmony': 558,\n",
       "  'gonna': 559,\n",
       "  'history': 560,\n",
       "  'compilation': 561,\n",
       "  'summer': 562,\n",
       "  'complete': 563,\n",
       "  'effect': 564,\n",
       "  'soft': 565,\n",
       "  'please': 566,\n",
       "  'soulful': 567,\n",
       "  'lyrically': 568,\n",
       "  'brings': 569,\n",
       "  '11': 570,\n",
       "  'recommended': 571,\n",
       "  'road': 572,\n",
       "  'write': 573,\n",
       "  'energy': 574,\n",
       "  'ear': 575,\n",
       "  'vibe': 576,\n",
       "  'filler': 577,\n",
       "  'lack': 578,\n",
       "  'mostly': 579,\n",
       "  'result': 580,\n",
       "  'contains': 581,\n",
       "  'stay': 582,\n",
       "  'standard': 583,\n",
       "  'tone': 584,\n",
       "  'major': 585,\n",
       "  'james': 586,\n",
       "  'plus': 587,\n",
       "  'snoop': 588,\n",
       "  'city': 589,\n",
       "  'return': 590,\n",
       "  'mainstream': 591,\n",
       "  'talking': 592,\n",
       "  'creative': 593,\n",
       "  'saw': 594,\n",
       "  'w': 595,\n",
       "  'em': 596,\n",
       "  'expect': 597,\n",
       "  'j': 598,\n",
       "  'guess': 599,\n",
       "  'knew': 600,\n",
       "  'crazy': 601,\n",
       "  'feat': 602,\n",
       "  'genius': 603,\n",
       "  'cry': 604,\n",
       "  'bone': 605,\n",
       "  'emotional': 606,\n",
       "  'message': 607,\n",
       "  'prince': 608,\n",
       "  'begin': 609,\n",
       "  'female': 610,\n",
       "  'scene': 611,\n",
       "  'perfectly': 612,\n",
       "  'period': 613,\n",
       "  'worst': 614,\n",
       "  'stevie': 615,\n",
       "  'project': 616,\n",
       "  'favourite': 617,\n",
       "  'nearly': 618,\n",
       "  'moody': 619,\n",
       "  'dead': 620,\n",
       "  'buying': 621,\n",
       "  'dj': 622,\n",
       "  'ten': 623,\n",
       "  'rush': 624,\n",
       "  '20': 625,\n",
       "  'clear': 626,\n",
       "  'disco': 627,\n",
       "  'seen': 628,\n",
       "  'element': 629,\n",
       "  'wanna': 630,\n",
       "  'enjoyable': 631,\n",
       "  'cent': 632,\n",
       "  'raw': 633,\n",
       "  'funny': 634,\n",
       "  'soon': 635,\n",
       "  '40': 636,\n",
       "  'boring': 637,\n",
       "  'essential': 638,\n",
       "  'deal': 639,\n",
       "  'wrote': 640,\n",
       "  'missing': 641,\n",
       "  'ya': 642,\n",
       "  'edition': 643,\n",
       "  'horn': 644,\n",
       "  'hey': 645,\n",
       "  'cash': 646,\n",
       "  'leave': 647,\n",
       "  'la': 648,\n",
       "  'average': 649,\n",
       "  'read': 650,\n",
       "  'totally': 651,\n",
       "  'step': 652,\n",
       "  'vocalist': 653,\n",
       "  'range': 654,\n",
       "  'biggest': 655,\n",
       "  'shine': 656,\n",
       "  'slightly': 657,\n",
       "  'felt': 658,\n",
       "  'act': 659,\n",
       "  'low': 660,\n",
       "  'reminds': 661,\n",
       "  'dr': 662,\n",
       "  'mellow': 663,\n",
       "  'soundtrack': 664,\n",
       "  'disappointed': 665,\n",
       "  'double': 666,\n",
       "  'serious': 667,\n",
       "  'standout': 668,\n",
       "  'wave': 669,\n",
       "  'duet': 670,\n",
       "  'reviewer': 671,\n",
       "  'edge': 672,\n",
       "  'mark': 673,\n",
       "  'kick': 674,\n",
       "  'alot': 675,\n",
       "  'van': 676,\n",
       "  'moving': 677,\n",
       "  'coast': 678,\n",
       "  'date': 679,\n",
       "  'simon': 680,\n",
       "  'except': 681,\n",
       "  'taking': 682,\n",
       "  'haunting': 683,\n",
       "  'final': 684,\n",
       "  'movie': 685,\n",
       "  'middle': 686,\n",
       "  'taste': 687,\n",
       "  'basically': 688,\n",
       "  'pac': 689,\n",
       "  'list': 690,\n",
       "  '60': 691,\n",
       "  'taken': 692,\n",
       "  'question': 693,\n",
       "  'da': 694,\n",
       "  'red': 695,\n",
       "  'none': 696,\n",
       "  'seemed': 697,\n",
       "  'yeah': 698,\n",
       "  'machine': 699,\n",
       "  'giving': 700,\n",
       "  'rain': 701,\n",
       "  'whether': 702,\n",
       "  'direction': 703,\n",
       "  'relationship': 704,\n",
       "  'guitarist': 705,\n",
       "  'beatles': 706,\n",
       "  'unfortunately': 707,\n",
       "  'state': 708,\n",
       "  'near': 709,\n",
       "  'concept': 710,\n",
       "  'water': 711,\n",
       "  'based': 712,\n",
       "  'whatever': 713,\n",
       "  'rich': 714,\n",
       "  'unit': 715,\n",
       "  'following': 716,\n",
       "  'unlike': 717,\n",
       "  'closer': 718,\n",
       "  'anthem': 719,\n",
       "  'drop': 720,\n",
       "  'beauty': 721,\n",
       "  '2pac': 722,\n",
       "  'exactly': 723,\n",
       "  'dogg': 724,\n",
       "  'decided': 725,\n",
       "  'okay': 726,\n",
       "  'enjoyed': 727,\n",
       "  'finest': 728,\n",
       "  'week': 729,\n",
       "  'sounded': 730,\n",
       "  'forget': 731,\n",
       "  'electric': 732,\n",
       "  'mike': 733,\n",
       "  'thug': 734,\n",
       "  'strange': 735,\n",
       "  'titled': 736,\n",
       "  'offer': 737,\n",
       "  'billy': 738,\n",
       "  'due': 739,\n",
       "  'mention': 740,\n",
       "  'wall': 741,\n",
       "  'joe': 742,\n",
       "  'beyond': 743,\n",
       "  'superb': 744,\n",
       "  'jack': 745,\n",
       "  'compared': 746,\n",
       "  'ability': 747,\n",
       "  'includes': 748,\n",
       "  'addition': 749,\n",
       "  'appreciate': 750,\n",
       "  'usually': 751,\n",
       "  'extremely': 752,\n",
       "  'war': 753,\n",
       "  'audience': 754,\n",
       "  'person': 755,\n",
       "  'inspired': 756,\n",
       "  'ride': 757,\n",
       "  'forever': 758,\n",
       "  'mine': 759,\n",
       "  'remastered': 760,\n",
       "  'brought': 761,\n",
       "  'electronic': 762,\n",
       "  'ice': 763,\n",
       "  'america': 764,\n",
       "  'featured': 765,\n",
       "  'rule': 766,\n",
       "  'legend': 767,\n",
       "  'section': 768,\n",
       "  'remains': 769,\n",
       "  'purchase': 770,\n",
       "  '15': 771,\n",
       "  'epic': 772,\n",
       "  'killer': 773,\n",
       "  'sun': 774,\n",
       "  'cause': 775,\n",
       "  'skill': 776,\n",
       "  'appearance': 777,\n",
       "  'tear': 778,\n",
       "  'month': 779,\n",
       "  'familiar': 780,\n",
       "  'admit': 781,\n",
       "  'successful': 782,\n",
       "  'mixed': 783,\n",
       "  'impressive': 784,\n",
       "  'master': 785,\n",
       "  'obvious': 786,\n",
       "  'brown': 787,\n",
       "  'lovely': 788,\n",
       "  'particularly': 789,\n",
       "  'changed': 790,\n",
       "  'phil': 791,\n",
       "  'hardcore': 792,\n",
       "  'usual': 793,\n",
       "  '13': 794,\n",
       "  'certain': 795,\n",
       "  'respect': 796,\n",
       "  'forward': 797,\n",
       "  'clearly': 798,\n",
       "  'choice': 799,\n",
       "  'followed': 800,\n",
       "  'living': 801,\n",
       "  'memory': 802,\n",
       "  'particular': 803,\n",
       "  'create': 804,\n",
       "  'surprise': 805,\n",
       "  'angel': 806,\n",
       "  'order': 807,\n",
       "  'truth': 808,\n",
       "  'contemporary': 809,\n",
       "  'morning': 810,\n",
       "  'glad': 811,\n",
       "  'picture': 812,\n",
       "  'david': 813,\n",
       "  'famous': 814,\n",
       "  'ready': 815,\n",
       "  'pain': 816,\n",
       "  'term': 817,\n",
       "  'mc': 818,\n",
       "  'room': 819,\n",
       "  'peace': 820,\n",
       "  'blood': 821,\n",
       "  'heaven': 822,\n",
       "  'everybody': 823,\n",
       "  'listens': 824,\n",
       "  'million': 825,\n",
       "  'upon': 826,\n",
       "  'continues': 827,\n",
       "  'across': 828,\n",
       "  'indeed': 829,\n",
       "  'recent': 830,\n",
       "  'hendrix': 831,\n",
       "  'earth': 832,\n",
       "  'important': 833,\n",
       "  'appeal': 834,\n",
       "  'thinking': 835,\n",
       "  'price': 836,\n",
       "  'waiting': 837,\n",
       "  'critic': 838,\n",
       "  'cat': 839,\n",
       "  'sex': 840,\n",
       "  'turned': 841,\n",
       "  'obviously': 842,\n",
       "  'stage': 843,\n",
       "  'thank': 844,\n",
       "  'rating': 845,\n",
       "  'tour': 846,\n",
       "  'walk': 847,\n",
       "  'sung': 848,\n",
       "  'blend': 849,\n",
       "  'cube': 850,\n",
       "  'amazon': 851,\n",
       "  'george': 852,\n",
       "  'filled': 853,\n",
       "  'present': 854,\n",
       "  'gun': 855,\n",
       "  'shot': 856,\n",
       "  'session': 857,\n",
       "  'magic': 858,\n",
       "  'mile': 859,\n",
       "  'within': 860,\n",
       "  'main': 861,\n",
       "  'cold': 862,\n",
       "  'exception': 863,\n",
       "  'dylan': 864,\n",
       "  'issue': 865,\n",
       "  'sarah': 866,\n",
       "  'jewel': 867,\n",
       "  'south': 868,\n",
       "  'comparison': 869,\n",
       "  'sell': 870,\n",
       "  'space': 871,\n",
       "  'delivery': 872,\n",
       "  'ray': 873,\n",
       "  'outstanding': 874,\n",
       "  'air': 875,\n",
       "  'longer': 876,\n",
       "  'towards': 877,\n",
       "  'vinyl': 878,\n",
       "  'x': 879,\n",
       "  'attempt': 880,\n",
       "  'opener': 881,\n",
       "  'co': 882,\n",
       "  'ja': 883,\n",
       "  'southern': 884,\n",
       "  'queen': 885,\n",
       "  'bridge': 886,\n",
       "  'skip': 887,\n",
       "  'indie': 888,\n",
       "  'dog': 889,\n",
       "  'born': 890,\n",
       "  'purple': 891,\n",
       "  'finish': 892,\n",
       "  'grow': 893,\n",
       "  'non': 894,\n",
       "  'consider': 895,\n",
       "  'starting': 896,\n",
       "  'driving': 897,\n",
       "  'deserves': 898,\n",
       "  'gorgeous': 899,\n",
       "  'credit': 900,\n",
       "  'chord': 901,\n",
       "  'laid': 902,\n",
       "  'book': 903,\n",
       "  'stick': 904,\n",
       "  'loud': 905,\n",
       "  'rapping': 906,\n",
       "  'latter': 907,\n",
       "  'synth': 908,\n",
       "  '14': 909,\n",
       "  'alive': 910,\n",
       "  'root': 911,\n",
       "  'cannot': 912,\n",
       "  'prof': 913,\n",
       "  'working': 914,\n",
       "  'approach': 915,\n",
       "  'drug': 916,\n",
       "  'showcase': 917,\n",
       "  'subject': 918,\n",
       "  'sold': 919,\n",
       "  'created': 920,\n",
       "  'produce': 921,\n",
       "  'latest': 922,\n",
       "  'sky': 923,\n",
       "  'tonight': 924,\n",
       "  'annoying': 925,\n",
       "  'wayne': 926,\n",
       "  'honest': 927,\n",
       "  'mccartney': 928,\n",
       "  'plenty': 929,\n",
       "  'family': 930,\n",
       "  'small': 931,\n",
       "  'son': 932,\n",
       "  'tha': 933,\n",
       "  'personally': 934,\n",
       "  'force': 935,\n",
       "  'originally': 936,\n",
       "  'dirty': 937,\n",
       "  'eagle': 938,\n",
       "  'moon': 939,\n",
       "  'beautifully': 940,\n",
       "  'uk': 941,\n",
       "  'front': 942,\n",
       "  'box': 943,\n",
       "  'rare': 944,\n",
       "  'sorry': 945,\n",
       "  'alternative': 946,\n",
       "  'extra': 947,\n",
       "  'instrumentation': 948,\n",
       "  'possibly': 949,\n",
       "  'backing': 950,\n",
       "  'shame': 951,\n",
       "  'rocking': 952,\n",
       "  'typical': 953,\n",
       "  'recently': 954,\n",
       "  'worthy': 955,\n",
       "  'tale': 956,\n",
       "  'biggie': 957,\n",
       "  'human': 958,\n",
       "  'mentioned': 959,\n",
       "  'compare': 960,\n",
       "  'surprised': 961,\n",
       "  'halen': 962,\n",
       "  'underground': 963,\n",
       "  'k': 964,\n",
       "  'added': 965,\n",
       "  'jazzy': 966,\n",
       "  'nobody': 967,\n",
       "  'alice': 968,\n",
       "  'kiss': 969,\n",
       "  'wild': 970,\n",
       "  'underrated': 971,\n",
       "  'dan': 972,\n",
       "  'thanks': 973,\n",
       "  'save': 974,\n",
       "  'influenced': 975,\n",
       "  'gospel': 976,\n",
       "  'pink': 977,\n",
       "  'honestly': 978,\n",
       "  'wind': 979,\n",
       "  'stunning': 980,\n",
       "  'mother': 981,\n",
       "  'york': 982,\n",
       "  'selling': 983,\n",
       "  'buck': 984,\n",
       "  'nature': 985,\n",
       "  'aside': 986,\n",
       "  'variety': 987,\n",
       "  'men': 988,\n",
       "  'spot': 989,\n",
       "  'jimmy': 990,\n",
       "  'joy': 991,\n",
       "  'gold': 992,\n",
       "  'considered': 993,\n",
       "  'skit': 994,\n",
       "  'town': 995,\n",
       "  'composition': 996,\n",
       "  'trip': 997,\n",
       "  'quiet': 998,\n",
       "  'immediately': 999,\n",
       "  'faith': 1000,\n",
       "  ...})"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_index(data=prep_df.words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbfbe24-d0d1-4b21-9093-aededebf11d5",
   "metadata": {},
   "source": [
    "#### c. proposed word embedding length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016708bc-40f2-4dbd-bef6-cdd55533b962",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a0032a5-70a5-4ec4-878a-40ddac51b8dc",
   "metadata": {},
   "source": [
    "#### d. statistical justification for the chosen maximum sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2efb1b8-bd2d-470d-9750-970703a2a56d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95e6e817-eaaa-46fd-839c-b1a0cb00563d",
   "metadata": {},
   "source": [
    "### 2.  Describe the goals of the tokenization process, including any code generated and packages that are used to normalize text during the tokenization process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbd1686-20f5-486d-82eb-36900ba0f7e3",
   "metadata": {},
   "source": [
    "### 3.  Explain the padding process used to standardize the length of sequences, including the following in your explanation:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c08a48-a990-4daa-b857-5a7a76f1109a",
   "metadata": {},
   "source": [
    "#### a. if the padding occurs before or after the text sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b91fcce-d35c-4674-926c-c7b949b595c3",
   "metadata": {},
   "source": [
    "#### b. a screenshot of a single padded sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b764cff8-c762-4e92-849d-207add8190c7",
   "metadata": {},
   "source": [
    "### 4.  Identify how many categories of sentiment will be used and an activation function for the final dense layer of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaf3fa2-ce68-4c02-b2db-d41483b2e3b1",
   "metadata": {},
   "source": [
    "### 5.  Explain the steps used to prepare the data for analysis, including the size of the training, validation, and test set split."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad84a30d-eeea-4f77-80d9-6903dec764fe",
   "metadata": {},
   "source": [
    "### 6.  Provide a copy of the prepared dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4c8a61-5107-474b-a2ff-fdb3146e74dd",
   "metadata": {},
   "source": [
    "# Part III:  Network Architecture\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de7275f-87b2-45f8-8dc6-0c6424738d68",
   "metadata": {},
   "source": [
    "## C.  Describe the type of network used by doing the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6ad2066-8609-4e20-8eaa-faaf77bd97ac",
   "metadata": {},
   "source": [
    "### 1.  Provide the output of the model summary of the function from TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ddb545e-9ece-4e87-8b2b-fb49e6cff79e",
   "metadata": {},
   "source": [
    "### 2.  Discuss the number of layers, the type of layers, and total number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d82b75-2ede-4601-a01e-6b23a405eeea",
   "metadata": {},
   "source": [
    "\n",
    "### 3.  Justify the choice of hyperparameters, including the following elements:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff93744-c21b-4d6d-9a89-bbbc84ed1379",
   "metadata": {},
   "source": [
    "\n",
    "#### activation functions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea11cd2-d89a-4757-8d26-02d73139d054",
   "metadata": {},
   "source": [
    "#### number of nodes per layer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74640e60-786c-4b46-9304-84fbecc2811e",
   "metadata": {},
   "source": [
    "#### loss function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c70d71a-5b6f-422f-ba46-b56e1d20f217",
   "metadata": {},
   "source": [
    "\n",
    "#### optimizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98448e48-3c69-4664-9905-437dc99eac7e",
   "metadata": {},
   "source": [
    "#### stopping criteria\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ec97f1-3fb5-4118-a08e-95ac6a0855db",
   "metadata": {},
   "source": [
    "#### evaluation metric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9b70d2-09bb-4dcc-8290-f5b7d649384d",
   "metadata": {},
   "source": [
    "# Part IV:  Model Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963a4800-d697-46f7-89d6-25fccd086822",
   "metadata": {},
   "source": [
    "## D.  Evaluate the model training process and its relevant outcomes by doing the following:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f87644-36be-46c2-a40f-892049b72461",
   "metadata": {},
   "source": [
    "### 1.  Discuss the impact of using stopping criteria instead of defining the number of epochs, including a screenshot showing the final training epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f6bc8b-d8b4-45a8-a9fd-c791e8853e3c",
   "metadata": {},
   "source": [
    "### 2.  Provide visualizations of the model’s training process, including a line graph of the loss and chosen evaluation metric."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd88d621-7db6-4184-9be3-330c5cab8be4",
   "metadata": {},
   "source": [
    "\n",
    "### 3.  Assess the fitness of the model and any measures taken to address overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23196a32-dae8-4471-b43c-62b4925d231b",
   "metadata": {},
   "source": [
    "\n",
    "### 4.  Discuss the predictive accuracy of the trained network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6203819-6583-4e6a-afa7-fa6d8f147500",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cff371e6-09e5-42fd-ba25-a2c93e6a3c87",
   "metadata": {},
   "source": [
    "# Part V:  Summary and Recommendations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcc2f22-708c-4ab1-b17f-61750b0d0149",
   "metadata": {},
   "source": [
    "## E.  Provide the code used to save the trained network within the neural network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5a978c7-f933-42ef-857b-a7fbb746fd50",
   "metadata": {},
   "source": [
    "## F.  Discuss the functionality of your neural network, including the impact of the network architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc5b3c8-6036-4f7f-8e80-e8fe6021bd42",
   "metadata": {},
   "source": [
    "## G.  Recommend a course of action based on your results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f183c122-5747-4c36-9950-9fb865359a45",
   "metadata": {},
   "source": [
    "# Part VI: Reporting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d5f95-73c6-451a-9005-a1f019d0779a",
   "metadata": {},
   "source": [
    "## H.  Create your neural network using an industry-relevant interactive development environment (e.g., a Jupyter Notebook). Include a PDF or HTML document of your executed notebook presentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dc577b-b770-40e2-bd67-62b200704e0f",
   "metadata": {},
   "source": [
    "## I.  List the web sources used to acquire data or segments of third-party code to support the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c181023-2ad6-4bbf-a637-c2f18a54bf99",
   "metadata": {},
   "source": [
    "## J.  Acknowledge sources, using in-text citations and references, for content that is quoted, paraphrased, or summarized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a7aa00-40d4-4cf2-a983-3d7c303d5b01",
   "metadata": {},
   "source": [
    "## K.  Demonstrate professional communication in the content and presentation of your submission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f48d2bc-ae9d-4f75-b7c1-0dcf0f0a7796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.9",
   "language": "python",
   "name": "py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
