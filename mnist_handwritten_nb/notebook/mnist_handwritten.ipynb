{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforms an image's numerical label to a vector of length 10 comprised of 0s,\n",
    "# except for the label's numerical index is represented as a 1\n",
    "# example: label \"5\" is represented as a one-hot encoded vector of [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
    "def one_hot_encode_label(label):\n",
    "    encoded_label = np.zeros((10, 1), dtype=int)\n",
    "    encoded_label[label] = 1\n",
    "    return encoded_label\n",
    "\n",
    "\n",
    "# create pairs of features and labels.\n",
    "\n",
    "\n",
    "def shape_data(data):\n",
    "\n",
    "    # flatten 28Ã—28 pixel input image into a 784 length vector\n",
    "    # vector values represents a grayscale value between 0 and 1\n",
    "    # 0 indicating white and 1 indicating black\n",
    "    features = [np.reshape(x, (784, 1)) for x in data[0]]\n",
    "    labels = [one_hot_encode_label(y) for y in data[1]]\n",
    "\n",
    "    return list(zip(features, labels))\n",
    "\n",
    "\n",
    "# unzip and unpickle the image data set, yielding the training and test data\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    with gzip.open(\"./data/mnist.pkl.gz\", \"rb\") as f:\n",
    "        train_data, validation_data, test_data = pickle.load(f, encoding=\"latin1\")\n",
    "        f.close()\n",
    "\n",
    "    return shape_data(train_data), shape_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifies numbers based on label and by averaging their 784 length vector\n",
    "# allows for comparing images based on an image's numerical average\n",
    "def average_digit(data, digit):\n",
    "    # filter values when the digit argument matches the index of the maximum value (the hot-encoded label) within the vector\n",
    "    filtered_data = [x[0] for x in data if np.argmax(x[1]) == digit]\n",
    "    filtered_array = np.asarray(filtered_data)\n",
    "    return np.average(filtered_array, axis=0)\n",
    "\n",
    "\n",
    "train, test = load_data()\n",
    "# use the average eight as parameters for a simple model to detect eights.\n",
    "avg_eight = average_digit(train, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img = np.reshape(avg_eight, (28, 28))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[20.066473]], dtype=float32), array([[54.196236]], dtype=float32))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num4 = train[2][0]\n",
    "num8 = train[17][0]\n",
    "\n",
    "weight = np.transpose(avg_eight)\n",
    "# dot product of the weight and image number vectors\n",
    "np.dot(weight, num4), np.dot(weight, num8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.4842492e-11]], dtype=float32), array([[0.99989855]], dtype=float32))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform the output of the dot product to the range [0, 1].\n",
    "# to define a cutoff value at 0.5 and declare everything above this value an 8.\n",
    "def sigmoid_double(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "# computes the sigmoid for vectors\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return np.vectorize(sigmoid_double)(z)\n",
    "\n",
    "\n",
    "# A simple prediction is defined by applying sigmoid to the output of np.doc(W, x) + b.\n",
    "\n",
    "\n",
    "def predict(x, weight, bias):\n",
    "    return sigmoid_double(np.dot(weight, x) + bias)\n",
    "\n",
    "\n",
    "# Based on the examples computed so far we set the bias term to -45.\n",
    "bias = -45\n",
    "predict(num4, weight, bias), predict(num8, weight, bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average dot products for each number\n",
    "# 0 = 45.965942\n",
    "# 1 = 30.596882\n",
    "# 2 = 44.91845\n",
    "# 3 = 45.77951\n",
    "# 4 = 38.09355\n",
    "# 5 = 41.33466\n",
    "# 6 = 41.18186\n",
    "# 7 = 35.819122\n",
    "# 8 = 54.832474\n",
    "# 9 = 41.69966"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As indicated before, you define a cutoff, or decision threshold to decide whether a prediction is counted as a given digit\n",
    "# use this evaluation function to assess the quality of predictions for data sets\n",
    "#  As evaluation metric we choose accuracy, the ratio of correct predictions among all.\n",
    "def evaluate(data, digit, threshold, weight, bias):\n",
    "    total_samples = 1.0 * len(data)\n",
    "    correct_predictions = 0\n",
    "\n",
    "    for x in data:\n",
    "        p = predict(x[0], weight, bias)\n",
    "        # Predicting an instance of an eight as \"8\" is a correct prediction.\n",
    "        if p > threshold and np.argmax(x[1]) == digit:\n",
    "            correct_predictions += 1\n",
    "        #  If the prediction is below our threshold and the sample is not an \"8\", we also predicted correctly.\n",
    "        if p <= threshold and np.argmax(x[1]) != digit:\n",
    "            correct_predictions += 1\n",
    "\n",
    "    return correct_predictions / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.68278, 0.6735, 0.8121149897330595)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threshold = 0.5\n",
    "\n",
    "# Accuracy on training data of our simple model is 68%\n",
    "e1 = evaluate(data=train, digit=8, threshold=threshold, weight=weight, bias=bias)\n",
    "\n",
    "# Accuracy on test data is slightly lower, at 67%\n",
    "e2 = evaluate(data=test, digit=8, threshold=threshold, weight=weight, bias=bias)\n",
    "\n",
    "eight_test = [x for x in test if np.argmax(x[1]) == 8]\n",
    "# Evaluating only on the set of eights in the test set only results in 81% accuracy\n",
    "e3 = evaluate(data=eight_test, digit=8, threshold=threshold, weight=weight, bias=bias)\n",
    "\n",
    "e1, e2, e3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# We use mean squared error as our loss function\n",
    "# You measure how close your prediction was to the actual label, by measuring the squared distance and averaging over all observed examples.\n",
    "# The loss function for a set of predictions and labels gives you information about how well tuned the parameters of your model are.\n",
    "# The smaller the loss, the better your predictions, and vice versa. The loss function itself is a function of the parameters of your network. In your MSE implementation, you donâ€™t directly supply weights, but theyâ€™re implicitly given through predictions, because you use weights to compute them.\n",
    "\n",
    "\n",
    "class MeanSquaredError:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_function(predictions, labels):\n",
    "        diff = predictions - labels\n",
    "        # defining MSE as 0.5 times the square difference between predictions and labels\n",
    "        return 0.5 * sum(diff * diff)[0]\n",
    "\n",
    "    @staticmethod\n",
    "    def loss_derivative(predictions, labels):\n",
    "        return predictions - labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_double(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "# computes the sigmoid for vectors\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return np.vectorize(sigmoid_double)(z)\n",
    "\n",
    "\n",
    "def sigmoid_prime_double(x):\n",
    "    return sigmoid_double(x) * (1 - sigmoid_double(x))\n",
    "\n",
    "\n",
    "# computes the sigmoid for vectors\n",
    "\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    return np.vectorize(sigmoid_prime_double)(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "deal with input data (the forward pass), but also a mechanism to propagate back error terms.\n",
    "In order not to recompute activation values on the backward pass,\n",
    "itâ€™s practical to maintain the state of data coming into and out of the layer for both passes.\n",
    "\n",
    "A layer has a list of parameters and stores both its current input and output data, as well as the respective input and output deltas for the backward pass. \n",
    "\"\"\"\n",
    "\n",
    "\n",
    "class Layer:\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "\n",
    "        # A layer knows its predecessor\n",
    "        self.previous = None\n",
    "        # A layer knows its successor\n",
    "        self.next = None\n",
    "\n",
    "        # Each layer can persist data flowing into it in the forward pass.\n",
    "        self.input_data = None\n",
    "        # Each layer can persist data flowing out of it in the forward pass.\n",
    "        self.output_data = None\n",
    "\n",
    "        self.input_delta = None\n",
    "        self.output_delta = None\n",
    "\n",
    "    # give each layer a successor and a predecessor to allow for layer sequencing\n",
    "    # This method connects a layer to its direct neighbours in the sequential network.\n",
    "    def connect(self, layer):\n",
    "        self.previous = layer\n",
    "        layer.next = self\n",
    "\n",
    "    # feed input data forward.\n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # nput_data is reserved for the first layer, all others get their input from the previous output.\n",
    "    def get_forward_input(self):\n",
    "        if self.previous is not None:\n",
    "            return self.previous.output_data\n",
    "        else:\n",
    "            return self.input_data\n",
    "\n",
    "    # Layers have to implement backpropagation of error terms, that is a way to feed input errors backward through the network.\n",
    "    def backward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # Input delta is reserved for the last layer, all other layers get their error terms from their successor.\n",
    "    def get_backward_input(self):\n",
    "        if self.next is not None:\n",
    "            return self.next.output_delta\n",
    "        else:\n",
    "            return self.input_delta\n",
    "\n",
    "    # reset deltas periodically, after accumulating deltas over mini-batches\n",
    "    # We compute and accumulate deltas per mini-batch, after which we need to reset these deltas.\n",
    "    def clear_deltas(self):\n",
    "        pass\n",
    "\n",
    "    # updating parameters for this layer, after the network using this layer tells it to\n",
    "    # Update layer parameters according to current deltas, using the specified learning_rate.\n",
    "    def update_params(self, learning_rate):\n",
    "        pass\n",
    "\n",
    "    def describe(self):\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This activation layer uses the sigmoid function to activate neurons.\n",
    "class ActivationLayer(Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super(ActivationLayer, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = input_dim\n",
    "\n",
    "    #  take an input data sample x and pass it through the network to arrive at a prediction\n",
    "    # applying the sigmoid to the input data.\n",
    "    def forward(self):\n",
    "        data = self.get_forward_input()\n",
    "        self.output_data = sigmoid(data)\n",
    "\n",
    "    # The backward pass is element-wise multiplication of the error term with the sigmoid derivative evaluated at the input to this layer.\n",
    "    def backward(self):\n",
    "        delta = self.get_backward_input()\n",
    "        data = self.get_forward_input()\n",
    "        self.output_delta = delta * sigmoid_prime(data)\n",
    "\n",
    "    def describe(self):\n",
    "        print(\"|-- \" + self.__class__.__name__)\n",
    "        print(\"  |-- dimensions: ({},{})\".format(self.input_dim, self.output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class DenseLayer(Layer):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DenseLayer, self).__init__()\n",
    "\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        # randomly initialize weight matrix and bias vector.\n",
    "        self.weight = np.random.randn(output_dim, input_dim)\n",
    "        self.bias = np.random.randn(output_dim, 1)\n",
    "\n",
    "        # The layer parameters consist of weights and bias terms.\n",
    "        self.params = [self.weight, self.bias]\n",
    "\n",
    "        # Deltas for weights and biases are set to zero.\n",
    "        self.delta_w = np.zeros(self.weight.shape)\n",
    "        self.delta_b = np.zeros(self.bias.shape)\n",
    "\n",
    "    # the affine linear transformation on input data defined by weights and biases.\n",
    "    def forward(self):\n",
    "        data = self.get_forward_input()\n",
    "        self.output_data = np.dot(self.weight, data) + self.bias\n",
    "\n",
    "    def backward(self):\n",
    "        # get input data and delta.\n",
    "        data = self.get_forward_input()\n",
    "        delta = self.get_backward_input()\n",
    "\n",
    "        # The current delta is added to the bias delta.\n",
    "        self.delta_b += delta\n",
    "\n",
    "        # add this term to the weight delta.\n",
    "        self.delta_w += np.dot(delta, data.transpose())\n",
    "\n",
    "        # The backward pass is completed by passing an output delta to the previous layer.\n",
    "        self.output_delta = np.dot(self.weight.transpose(), delta)\n",
    "\n",
    "    # Using weight and bias deltas we can update model parameters with gradient descent.\n",
    "    def update_params(self, rate):\n",
    "        self.weight -= rate * self.delta_w\n",
    "        self.bias -= rate * self.delta_b\n",
    "\n",
    "    # After updating parameters we should reset all deltas.\n",
    "    def clear_deltas(self):\n",
    "        self.delta_w = np.zeros(self.weight.shape)\n",
    "        self.delta_b = np.zeros(self.bias.shape)\n",
    "\n",
    "    def describe(self):\n",
    "        print(\"|--- \" + self.__class__.__name__)\n",
    "        print(\"  |-- dimensions: ({},{})\".format(self.input_dim, self.output_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequential neural network we stack layers sequentially.\n",
    "class SequentialNetwork:\n",
    "    def __init__(self, loss=None):\n",
    "        print(\"Initialize Network...\")\n",
    "        self.layers = []\n",
    "        # If no loss function is provided, MSE is used.\n",
    "        if loss is None:\n",
    "            self.loss = MeanSquaredError()\n",
    "\n",
    "    # Whenever we add a layer, we connect it to its predecessor and let it describe itself.\n",
    "    def add(self, layer):\n",
    "        self.layers.append(layer)\n",
    "        layer.describe()\n",
    "        if len(self.layers) > 1:\n",
    "            self.layers[-1].connect(self.layers[-2])\n",
    "\n",
    "    # To train our network, we pass over data for as many times as there are epochs.\n",
    "    def train(\n",
    "        self, training_data, epochs, mini_batch_size, learning_rate, test_data=None\n",
    "    ):\n",
    "        n = len(training_data)\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            # We shuffle training data and create mini-batches.\n",
    "            mini_batches = [\n",
    "                training_data[k : k + mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)\n",
    "            ]\n",
    "\n",
    "            # For each mini-batch we train our network.\n",
    "            for mini_batch in mini_batches:\n",
    "                self.train_batch(mini_batch, learning_rate)\n",
    "\n",
    "            if test_data:\n",
    "                n_test = len(test_data)\n",
    "                print(\n",
    "                    \"Epoch {0}: {1} / {2}\".format(\n",
    "                        epoch, self.evaluate(test_data), n_test\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                print(f\"Epoch {epoch} complete\")\n",
    "\n",
    "    # To train the network on a mini-batch, we compute feed-forward and backward pass.\n",
    "    # and then update model parameters accordingly.\n",
    "    def train_batch(self, mini_batch, learning_rate):\n",
    "        self.forward_backward(mini_batch)\n",
    "\n",
    "        self.update(mini_batch, learning_rate)\n",
    "\n",
    "    def update(self, mini_batch, learning_rate):\n",
    "        # normalize the learning rate by the mini-batch size.\n",
    "        learning_rate = learning_rate / len(mini_batch)\n",
    "        for layer in self.layers:\n",
    "            layer.update_params(learning_rate)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            layer.clear_deltas()\n",
    "\n",
    "    def forward_backward(self, mini_batch):\n",
    "        for x, y in mini_batch:\n",
    "            self.layers[0].input_data = x\n",
    "            # For each sample in the mini batch, feed the features forward layer by layer.\n",
    "            for layer in self.layers:\n",
    "                layer.forward()\n",
    "\n",
    "            # compute the loss derivative for the output data.\n",
    "            self.layers[-1].input_delta = self.loss.loss_derivative(\n",
    "                self.layers[-1].output_data, y\n",
    "            )\n",
    "\n",
    "            # layer-by-layer backpropagation of error terms.\n",
    "            for layer in reversed(self.layers):\n",
    "                layer.backward()\n",
    "\n",
    "    # Pass a single sample forward and return the result.\n",
    "    def single_forward(self, x):\n",
    "        self.layers[0].input_data = x\n",
    "        for layer in self.layers:\n",
    "            layer.forward()\n",
    "        return self.layers[-1].output_data\n",
    "\n",
    "    # In case we provided test data, we evaluate our network on it after each epoch.\n",
    "    def evaluate(self, test_data):\n",
    "        test_results = [\n",
    "            (np.argmax(self.single_forward(x)), np.argmax(y)) for (x, y) in test_data\n",
    "        ]\n",
    "\n",
    "        return sum(int(x == y) for (x, y) in test_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize Network...\n",
      "|--- DenseLayer\n",
      "  |-- dimensions: (784,392)\n",
      "|-- ActivationLayer\n",
      "  |-- dimensions: (392,392)\n",
      "|--- DenseLayer\n",
      "  |-- dimensions: (392,196)\n",
      "|-- ActivationLayer\n",
      "  |-- dimensions: (196,196)\n",
      "|--- DenseLayer\n",
      "  |-- dimensions: (196,10)\n",
      "|-- ActivationLayer\n",
      "  |-- dimensions: (10,10)\n"
     ]
    }
   ],
   "source": [
    "training_data, test_data = load_data()\n",
    "\n",
    "net = SequentialNetwork()\n",
    "\n",
    "# To build a network, keep in mind that your input dimension is 784 and your output dimension is 10, the number of digits.\n",
    "# You choose three dense layers with output dimensions 392, 196, and 10, respectively, and add sigmoid activations after each of them.\n",
    "# With each new dense layer, you are effectively dividing layer capacity in half.\n",
    "# The layer sizes and the number of layers are hyperparameters for this network.\n",
    "# Youâ€™ve chosen these values to set up a network architecture.\n",
    "# We encourage you to experiment with other layer sizes to gain intuition about the learning process of a network in relation to its architecture.\n",
    "net.add(DenseLayer(784, 392))\n",
    "net.add(ActivationLayer(392))\n",
    "net.add(DenseLayer(392, 196))\n",
    "net.add(ActivationLayer(196))\n",
    "net.add(DenseLayer(196, 10))\n",
    "net.add(ActivationLayer(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 6723 / 10000\n",
      "Epoch 1: 8418 / 10000\n",
      "Epoch 2: 8522 / 10000\n",
      "Epoch 3: 8607 / 10000\n",
      "Epoch 4: 8616 / 10000\n",
      "Epoch 5: 8678 / 10000\n",
      "Epoch 6: 8703 / 10000\n",
      "Epoch 7: 8708 / 10000\n",
      "Epoch 8: 8743 / 10000\n",
      "Epoch 9: 8767 / 10000\n"
     ]
    }
   ],
   "source": [
    "# You train the network on data by calling train with all required parameters.\n",
    "# You run training for 10 epochs and set the learning rate to 3.0.\n",
    "# As mini-batch size, you choose 10, the number of classes.\n",
    "# If you were to shuffle training data near perfectly, in most batches each class would be represented, leading to good stochastic gradients.\n",
    "net.train(\n",
    "    training_data, epochs=10, mini_batch_size=10, learning_rate=3.0, test_data=test_data\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
